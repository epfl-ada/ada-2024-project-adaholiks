{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T13:59:02.213978Z",
     "start_time": "2024-12-03T13:59:01.998390Z"
    }
   },
   "source": [
    "import pyarrow.feather as feather\n",
    "import pandas as pd\n",
    "from src.utils import *"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring Methods for Game Path Analysis  \n",
    "\n",
    "***Prioritizing Scores Based on Minimal Clicks*** \n",
    "\n",
    "### **Difference Between Played Path Length and Optimal Distance**  \n",
    "- **Optimal Distance**: The shortest possible distance from the start to the target article.  \n",
    "- **Played Path Length**: The actual number of clicks (or visited articles - 1). This is represented in the dataset as `simplified_path_length`.  \n",
    "- Why use `simplified_path_length` instead of `full_path_length`?  \n",
    "  Simplified paths eliminate detours, thus only the articles relevant in finishing the path are taken into account.\n",
    "\n",
    "We define the **Path Score** for a completed path as:  \n",
    "$$\n",
    "\\mathbf{Path\\ Score} = \\frac{\\text{Optimal\\ Distance}}{\\text{Simplified\\ Path\\ Length}}\n",
    "$$  \n",
    "\n",
    "This score ranges from 0 to 1, where 1 indicates the closest adherence to the optimal path. We refer to this score as the **path weight**, representing the ratio of actual path length to optimal distance.\n",
    "\n",
    "#### **Article Scoring Based on Path Weights**  \n",
    "After computing path weights for all completed paths, we use them to derive article scores using two approaches:\n",
    "\n",
    "1. **Weighted Average**  \n",
    "   Compute the average path weight for each article across all paths it appears in:  \n",
    "   $$\n",
    "   \\mathbf{Article\\ Score} = \\frac{\\sum_{i=1}^n w_i}{n}\n",
    "   $$  \n",
    "   where  $w_1, w_2, \\dots, w_n$ are the path weights, and  $n$ is the number of paths the article appears in.  \n",
    "   - This score is about article quality over quantity.\n",
    "   - Only articles with a minimum appearance threshold are included to ensure meaningful scores.  \n",
    "   - **Function**: `calculate_avg_article_weights(df, count_cutoff=30, scaling=None)`  \n",
    "\n",
    "2. **Sum of Centered Weights**  \n",
    "   - **Centering**: First, compute the mean article weight across all paths:  \n",
    "     $$\n",
    "     \\text{Mean\\ Article\\ Weight} = \\frac{\\sum_{i=1}^N (\\text{path}_i\\ \\text{weight} \\times \\text{num\\_artcicles\\_in\\_path}_i)}{\\sum_{i=1}^N \\text{num\\_artcicles\\_in\\_path}_i}\n",
    "     $$  \n",
    "     where $N$ is the total number of paths (or a downsampled subset), and $\\text{num\\_artcicles\\_in\\_path}_i$ the number of articles in simplified path $i$ (without start and target article).\n",
    "\n",
    "   - **Centered Weights**:  \n",
    "     $$\n",
    "     \\mathbf{Centered\\ Weight} = \\mathbf{Path\\ Score} - \\text{Mean\\ Article\\ Weight}\n",
    "     $$  \n",
    "     Why center using article weight and not path weight? Because in the end, we are interested in computing article weights, and since paths don't have the same number of articles, the average path weight is not the same as the average article weight.\n",
    "\n",
    "   - Compute the article score by summing all centered weights for the paths the article appears in:  \n",
    "     $$\n",
    "     \\mathbf{Article\\ Score} = \\sum_{i=1}^n cw_i\n",
    "     $$  \n",
    "     where $cw_1, cw_2, \\dots, cw_n$ are the centered weights.  \n",
    "   - This score balances quality and usefulness within the game.  \n",
    "   - Only articles with a minimum appearance threshold are included to ensure meaningful scores.  \n",
    "   - **Function**: `calculate_sum_article_cweights(df, count_cutoff=30, scaling=None)`\n",
    "\n",
    "---\n",
    "\n",
    "## Scores Based on Article Appearance in Detours  \n",
    "\n",
    "### **Detour Ratio**  \n",
    "- Detours occur when articles are backtracked (i.e don't appear in simplified paths).  \n",
    "- For each article  $i$, the **Detour Ratio** is:  \n",
    "  $$\n",
    "  \\mathbf{DetourRatio_i} = \\frac{\\text{detour\\_count}_i}{\\text{total\\_appearances}_i},\n",
    "  $$  \n",
    "\n",
    "  where the $\\text{detour\\_count}_i$ and $\\text{total\\_appearances}_i$ are the number of appearance in detours and total number of apperances for article $i$ respectively.\n",
    "- Only articles with a minimum total appearance threshold are considered.  \n",
    "- **Function**: `calculate_detour_ratios(df, count_cutoff=30, scaling=None)`  \n",
    "\n",
    "---\n",
    "\n",
    "## Scores Based on Article Presence in Unfinished Paths  \n",
    "\n",
    "### **Unfinished Ratio**  \n",
    "- Measures how frequently an article appears in incomplete paths.  \n",
    "- For each article $i$, the **Unfinished Ratio** is:  \n",
    "  $$\n",
    "  \\mathbf{UnfinishedRatio_i} = \\frac{\\text{unfinished\\_count}_i}{\\text{total\\_appearances}_i}\n",
    "  $$  \n",
    "- Again, articles must meet a minimum appearance threshold for meaningful scores.  \n",
    "- **Function**: `calculate_unfinished_ratios(df, count_cutoff=30, scaling=None)`  \n",
    "\n",
    "**========================================================================================================================================**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Now Consider Scores That Reward Finishing the Game as Fast as Possible***\n",
    "\n",
    "### **Weighted Average of Article Speed**  \n",
    "We first compute **path speed**, defined as the time taken to complete the path (from `durationInSec`) divided by `full_path_length`. Similar to the weighted average of path weights, we can compute the average speed for each article. This involves extracting all $n$ paths an article appears in and calculating the average of the associated path speeds $s_1, s_2, \\dots, s_n$:  \n",
    "$$\n",
    "\\mathbf{Article\\ Speed} = \\frac{\\sum_{i=1}^n s_i}{n}\n",
    "$$  \n",
    "\n",
    "Where $s_1, s_2, \\dots, s_n$ are the path speeds, and $n$ is the number of paths containing the article.  \n",
    "\n",
    "- Only articles with a minimum total appearance threshold are included for meaningful scores.  \n",
    "- **Function**: `calc_avg_article_speed(df, count_cutoff=30, scaling=None)`  \n",
    "\n",
    "---\n",
    "\n",
    "### **Sum of Centered Article Speed**  \n",
    "This approach mirrors the **sum of centered weights** but uses **path speed** instead of path weight.  \n",
    "\n",
    "1. **Centering**: Compute the mean path speed across all paths:  \n",
    "   $$\n",
    "   \\text{Mean\\ Path\\ Speed} = \\frac{\\sum_{i=1}^N (\\text{path}_i\\ \\text{speed} \\times \\text{full\\_path\\_length}_i)}{\\sum_{i=1}^N \\text{full\\_path\\_length}_i}\n",
    "   $$  \n",
    "   where $N$ is the total number of paths (or a downsampled subset).  \n",
    "\n",
    "2. **Centered Speeds**:  \n",
    "   $$\n",
    "   \\mathbf{Centered\\ Speed} = \\mathbf{Path\\ Speed} - \\text{Mean\\ Path\\ Speed}\n",
    "   $$  \n",
    "\n",
    "3. Compute the article score by summing all centered speeds for the paths the article appears in:  \n",
    "   $$\n",
    "   \\mathbf{Article\\ Score} = \\sum_{i=1}^n cs_i\n",
    "   $$  \n",
    "   where $cs_1, cs_2, \\dots, cs_n$ are the centered speeds, and $n$ is the number of paths the article appears in.  \n",
    "\n",
    "- This score provides a balance between speed quality and frequency of appearance.  \n",
    "- **Function**: `calc_sum_article_cspeed(df, count_cutoff=30, scaling=None)`  \n",
    "\n",
    "---  \n",
    "***Imporant note about the scaling***  \n",
    "The functions are all coded in a way that when scaling is applied to the scores, large values always are better. So for example, even if the ratio of unfinished paths should be as small as possible, the sacled score column, is flipped, so that larger means better. This way when different scores are combined in a composite score, bigger is also always better.\n",
    "\n",
    "---  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now some example of how to get the scores.\n",
    "\n",
    "First need to load the filtered_paths and "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:07:52.873795Z",
     "start_time": "2024-12-03T14:07:52.069186Z"
    }
   },
   "source": [
    "filtered_paths = feather.read_feather('Data/dataframes/filtered_paths.feather')\n",
    "finished_paths = filtered_paths[filtered_paths['finished']]\n",
    "\n",
    "# downsample data to one IpAdress per identifier\n",
    "# this way players can't just learn paths and then play them as fast as possible\n",
    "# this is part of the data filtering process\n",
    "finished_paths = finished_paths.groupby(['hashedIpAddress', 'identifier']).sample(n=1, random_state=42)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then compute the scores prioritizing minimal number of clicks"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:08:04.447877Z",
     "start_time": "2024-12-03T14:08:04.432965Z"
    }
   },
   "source": [
    "avg_weight_df = calculate_avg_article_weights(finished_paths, count_cutoff=30, scaling='standard')\n",
    "unfinished_ratio_df = calculate_unfinished_ratios(filtered_paths, count_cutoff=30, scaling='standard')\n",
    "detour_ratio_df = calculate_detour_ratios(finished_paths, count_cutoff=30, scaling='standard')\n",
    "\n",
    "# for this one it might make sense to downsample the data set so that only one start target pair sample is present (the same way I do it Notebook_P2)\n",
    "sum_cweight_df = calculate_sum_article_cweights(finished_paths, count_cutoff=30, scaling='standard')\n",
    "\n",
    "# there are a bunch of prints which can be removed in the final version"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'calculate_avg_article_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m avg_weight_df \u001B[38;5;241m=\u001B[39m \u001B[43mcalculate_avg_article_weights\u001B[49m(finished_paths, count_cutoff\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m30\u001B[39m, scaling\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstandard\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      2\u001B[0m unfinished_ratio_df \u001B[38;5;241m=\u001B[39m calculate_unfinished_ratios(filtered_paths, count_cutoff\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m30\u001B[39m, scaling\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstandard\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      3\u001B[0m detour_ratio_df \u001B[38;5;241m=\u001B[39m calculate_detour_ratios(finished_paths, count_cutoff\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m30\u001B[39m, scaling\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstandard\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'calculate_avg_article_weights' is not defined"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the speed related ones"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T13:55:41.270313Z",
     "start_time": "2024-12-03T13:55:41.250729Z"
    }
   },
   "source": [
    "# need adtional speed filtering\n",
    "speed_filt_finished = filter_duration(finished_paths)\n",
    "\n",
    "# get the score dfs\n",
    "avg_speed_df = calc_avg_article_speed(speed_filt_finished, count_cutoff=30, scaling='standard')\n",
    "sum_scpeed_df = calc_sum_article_cspeed(speed_filt_finished, count_cutoff=30, scaling='standard')"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filter_duration' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# need adtional speed filtering\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m speed_filt_finished \u001B[38;5;241m=\u001B[39m \u001B[43mfilter_duration\u001B[49m(finished_paths)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# get the score dfs\u001B[39;00m\n\u001B[1;32m      5\u001B[0m avg_speed_df \u001B[38;5;241m=\u001B[39m calc_avg_article_speed(speed_filt_finished, count_cutoff\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m30\u001B[39m, scaling\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstandard\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'filter_duration' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All individual score dfs have the same (or very similar) format, something like: \n",
    "\n",
    " | article (index) | n_apperances | raw_score | scaled_score |\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_appearances</th>\n",
       "      <th>weighted_avg</th>\n",
       "      <th>standard</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Achilles</th>\n",
       "      <td>47</td>\n",
       "      <td>0.834448</td>\n",
       "      <td>4.811790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>J._K._Rowling</th>\n",
       "      <td>55</td>\n",
       "      <td>0.796696</td>\n",
       "      <td>4.044191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mario</th>\n",
       "      <td>38</td>\n",
       "      <td>0.764568</td>\n",
       "      <td>3.390951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Harry_Potter</th>\n",
       "      <td>62</td>\n",
       "      <td>0.754634</td>\n",
       "      <td>3.188972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lead</th>\n",
       "      <td>30</td>\n",
       "      <td>0.753016</td>\n",
       "      <td>3.156074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anatomy</th>\n",
       "      <td>40</td>\n",
       "      <td>0.485813</td>\n",
       "      <td>-2.276821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Irrigation</th>\n",
       "      <td>67</td>\n",
       "      <td>0.480171</td>\n",
       "      <td>-2.391556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gas</th>\n",
       "      <td>45</td>\n",
       "      <td>0.472884</td>\n",
       "      <td>-2.539718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Atheism</th>\n",
       "      <td>33</td>\n",
       "      <td>0.472619</td>\n",
       "      <td>-2.545097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actor</th>\n",
       "      <td>60</td>\n",
       "      <td>0.463882</td>\n",
       "      <td>-2.722738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>820 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               n_appearances  weighted_avg  standard\n",
       "article                                             \n",
       "Achilles                  47      0.834448  4.811790\n",
       "J._K._Rowling             55      0.796696  4.044191\n",
       "Mario                     38      0.764568  3.390951\n",
       "Harry_Potter              62      0.754634  3.188972\n",
       "Lead                      30      0.753016  3.156074\n",
       "...                      ...           ...       ...\n",
       "Anatomy                   40      0.485813 -2.276821\n",
       "Irrigation                67      0.480171 -2.391556\n",
       "Gas                       45      0.472884 -2.539718\n",
       "Atheism                   33      0.472619 -2.545097\n",
       "Actor                     60      0.463882 -2.722738\n",
       "\n",
       "[820 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_weight_df.sort_values(by='standard', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform statistical analysis, and particularly machine learning, the scaled score should be used. Scaling parameter options can be found in function docstrings. However, don't forget that when we do machine learning, to prevent data leakage we should first perform a train test splitt, and then do scaling. So in that case just work with raw scores as labels then slpit them and then scale.\n",
    "\n",
    "You can also make a df with some scores for comparison, and compute composite scores to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight_avg</th>\n",
       "      <th>weight_avg_scaled</th>\n",
       "      <th>unfinished_ratio</th>\n",
       "      <th>unf_ratio_scaled</th>\n",
       "      <th>detour_ratio</th>\n",
       "      <th>detour_ratio_scaled</th>\n",
       "      <th>comp_score_3</th>\n",
       "      <th>comp_score_2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Achilles</th>\n",
       "      <td>0.834448</td>\n",
       "      <td>4.811790</td>\n",
       "      <td>0.131148</td>\n",
       "      <td>0.850784</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.312944</td>\n",
       "      <td>4.158275</td>\n",
       "      <td>4.630274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>J._K._Rowling</th>\n",
       "      <td>0.796696</td>\n",
       "      <td>4.044191</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>1.006611</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>0.949604</td>\n",
       "      <td>3.559448</td>\n",
       "      <td>3.794620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algebra</th>\n",
       "      <td>0.716872</td>\n",
       "      <td>2.421177</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>1.664997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.312944</td>\n",
       "      <td>3.068307</td>\n",
       "      <td>2.686373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Harry_Potter</th>\n",
       "      <td>0.754634</td>\n",
       "      <td>3.188972</td>\n",
       "      <td>0.155844</td>\n",
       "      <td>0.565727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.312944</td>\n",
       "      <td>2.952162</td>\n",
       "      <td>3.310698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parrot</th>\n",
       "      <td>0.711134</td>\n",
       "      <td>2.304506</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>1.180703</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.312944</td>\n",
       "      <td>2.723494</td>\n",
       "      <td>2.591504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sport</th>\n",
       "      <td>0.550422</td>\n",
       "      <td>-0.963159</td>\n",
       "      <td>0.512563</td>\n",
       "      <td>-3.551640</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>-2.288856</td>\n",
       "      <td>-3.903475</td>\n",
       "      <td>-2.162060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVD</th>\n",
       "      <td>0.544194</td>\n",
       "      <td>-1.089802</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>-2.182449</td>\n",
       "      <td>0.254902</td>\n",
       "      <td>-3.966165</td>\n",
       "      <td>-4.100757</td>\n",
       "      <td>-3.241349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eukaryote</th>\n",
       "      <td>0.521525</td>\n",
       "      <td>-1.550714</td>\n",
       "      <td>0.432836</td>\n",
       "      <td>-2.631404</td>\n",
       "      <td>0.229167</td>\n",
       "      <td>-3.433178</td>\n",
       "      <td>-4.369271</td>\n",
       "      <td>-3.305899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Optical_fiber</th>\n",
       "      <td>0.512302</td>\n",
       "      <td>-1.738252</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>-0.867322</td>\n",
       "      <td>0.339286</td>\n",
       "      <td>-5.713781</td>\n",
       "      <td>-4.701717</td>\n",
       "      <td>-4.785863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actor</th>\n",
       "      <td>0.463882</td>\n",
       "      <td>-2.722738</td>\n",
       "      <td>0.536232</td>\n",
       "      <td>-3.824837</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>-1.563493</td>\n",
       "      <td>-4.811791</td>\n",
       "      <td>-3.170631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>820 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               weight_avg  weight_avg_scaled  unfinished_ratio  \\\n",
       "article                                                          \n",
       "Achilles         0.834448           4.811790          0.131148   \n",
       "J._K._Rowling    0.796696           4.044191          0.117647   \n",
       "Algebra          0.716872           2.421177          0.060606   \n",
       "Harry_Potter     0.754634           3.188972          0.155844   \n",
       "Parrot           0.711134           2.304506          0.102564   \n",
       "...                   ...                ...               ...   \n",
       "Sport            0.550422          -0.963159          0.512563   \n",
       "DVD              0.544194          -1.089802          0.393939   \n",
       "Eukaryote        0.521525          -1.550714          0.432836   \n",
       "Optical_fiber    0.512302          -1.738252          0.280000   \n",
       "Actor            0.463882          -2.722738          0.536232   \n",
       "\n",
       "               unf_ratio_scaled  detour_ratio  detour_ratio_scaled  \\\n",
       "article                                                              \n",
       "Achilles               0.850784      0.000000             1.312944   \n",
       "J._K._Rowling          1.006611      0.017544             0.949604   \n",
       "Algebra                1.664997      0.000000             1.312944   \n",
       "Harry_Potter           0.565727      0.000000             1.312944   \n",
       "Parrot                 1.180703      0.000000             1.312944   \n",
       "...                         ...           ...                  ...   \n",
       "Sport                 -3.551640      0.173913            -2.288856   \n",
       "DVD                   -2.182449      0.254902            -3.966165   \n",
       "Eukaryote             -2.631404      0.229167            -3.433178   \n",
       "Optical_fiber         -0.867322      0.339286            -5.713781   \n",
       "Actor                 -3.824837      0.138889            -1.563493   \n",
       "\n",
       "               comp_score_3  comp_score_2  \n",
       "article                                    \n",
       "Achilles           4.158275      4.630274  \n",
       "J._K._Rowling      3.559448      3.794620  \n",
       "Algebra            3.068307      2.686373  \n",
       "Harry_Potter       2.952162      3.310698  \n",
       "Parrot             2.723494      2.591504  \n",
       "...                     ...           ...  \n",
       "Sport             -3.903475     -2.162060  \n",
       "DVD               -4.100757     -3.241349  \n",
       "Eukaryote         -4.369271     -3.305899  \n",
       "Optical_fiber     -4.701717     -4.785863  \n",
       "Actor             -4.811791     -3.170631  \n",
       "\n",
       "[820 rows x 8 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Combine the metrics into one DataFrame\n",
    "composite_df = pd.DataFrame(index=avg_weight_df.index)\n",
    "composite_df['weight_avg'] = avg_weight_df['weighted_avg']\n",
    "composite_df['weight_avg_scaled'] = avg_weight_df['standard']\n",
    "\n",
    "composite_df['unfinished_ratio'] = unfinished_ratio_df['unfinished_ratio']\n",
    "composite_df['unf_ratio_scaled'] = unfinished_ratio_df['standard']\n",
    "\n",
    "composite_df['detour_ratio'] = detour_ratio_df['detour_ratio']\n",
    "composite_df['detour_ratio_scaled'] = detour_ratio_df['standard']\n",
    "\n",
    "# Compute composite score using PCA for all three scaled metrics\n",
    "pca = PCA(n_components=1)\n",
    "composite_df['comp_score_3'] = pca.fit_transform(\n",
    "    composite_df[['weight_avg_scaled', 'unf_ratio_scaled', 'detour_ratio_scaled']]\n",
    ")\n",
    "\n",
    "# Compute composite score using PCA for only weight_avg_scaled and detour_ratio_scaled\n",
    "pca = PCA(n_components=1)\n",
    "composite_df['comp_score_2'] = pca.fit_transform(\n",
    "    composite_df[['weight_avg_scaled', 'detour_ratio_scaled']]\n",
    ")\n",
    "\n",
    "# Sort by the composite score (correct column name)\n",
    "composite_df = composite_df.sort_values(by='comp_score_3', ascending=False)\n",
    "composite_df.to_feather('Data/dataframes/composite_scores_df.feather')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What (composite) scores do I suggest.\n",
    "\n",
    "Of course there are a lot of different combinations that can be tried out. Also a lot of different parameter as input for the function. Before doing statistical analysis a reasonable threshold should be agreed on (30 is kinda arbitrary). As for scaling, I think it is worth trying both the standard and minmax.\n",
    "\n",
    "**I would start with the following scores**\n",
    "- Only article average weights => `avg_weight_df['standard']`\n",
    "- Article average weights + detour ratios => `composite_df['comp_score_2']`\n",
    "- Sum of centered average weights => `sum_cweight_df['standard']`\n",
    "- Only average article speed => `avg_speed_df['standard']`\n",
    "\n",
    "(`'standard'` would become `'minmax'` if that is used for scaling)\n",
    "\n",
    "\n",
    "Since there are so many other combinations that might make sense, we can discuss that together to make sure in the end we have the most meaningfull thing.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADAproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
