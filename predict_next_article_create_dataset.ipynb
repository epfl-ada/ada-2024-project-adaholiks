{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-13 09:39:16.078190: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-13 09:39:16.245757: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734079156.335189   55036 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734079156.362779   55036 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-13 09:39:16.494612: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "2.18.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'current_article_text': 'Full text of Article A...',\n",
       " 'possible_next_articles': ['Article B', 'Article C', 'Article D'],\n",
       " 'target_article': 'Article D',\n",
       " 'final_target_article': 'Article Z'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataset  of the format\n",
    "{\n",
    "  \"current_article_text\": \"Full text of Article A...\",\n",
    "  \"possible_next_articles\": [\"Article B\", \"Article C\", \"Article D\"],\n",
    "  \"target_article\": \"Article D\",\n",
    "  \"final_target_article\": \"Article Z\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data = pd.read_feather('Data/dataframes/article_dataframe.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data['linkTarget']\n",
    "\n",
    "# Ge max number of links\n",
    "med_links = article_data['linkTarget'].apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4604.000000\n",
       "mean       26.038662\n",
       "std        24.201491\n",
       "min         0.000000\n",
       "25%        11.000000\n",
       "50%        19.000000\n",
       "75%        33.000000\n",
       "max       294.000000\n",
       "Name: linkTarget, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the paths\n",
    "paths_df = pd.read_feather('Data/dataframes/paths.feather')\n",
    "# only include successful paths\n",
    "paths_df = paths_df[paths_df['finished']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['hashedIpAddress', 'timestamp', 'durationInSec', 'path', 'rating',\n",
       "       'finished', 'failure_reason', 'start_article', 'target_article'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    path  \\\n",
      "0      14th_century;15th_century;16th_century;Pacific...   \n",
      "1      14th_century;Europe;Africa;Atlantic_slave_trad...   \n",
      "2      14th_century;Niger;Nigeria;British_Empire;Slav...   \n",
      "3         14th_century;Renaissance;Ancient_Greece;Greece   \n",
      "4      14th_century;Italy;Roman_Catholic_Church;HIV;R...   \n",
      "...                                                  ...   \n",
      "51313                   Yagan;Ancient_Egypt;Civilization   \n",
      "51314  Yagan;Folklore;Brothers_Grimm;<;19th_century;C...   \n",
      "51315  Yagan;Australia;England;France;United_States;T...   \n",
      "51316  Yarralumla,_Australian_Capital_Territory;Austr...   \n",
      "51317                            Ziad_Jarrah;Germany;Jew   \n",
      "\n",
      "                                          processed_path  \n",
      "0      14th_century;15th_century;16th_century;Pacific...  \n",
      "1      14th_century;Europe;Africa;Atlantic_slave_trad...  \n",
      "2      14th_century;Niger;Nigeria;British_Empire;Slav...  \n",
      "3         14th_century;Renaissance;Ancient_Greece;Greece  \n",
      "4      14th_century;Italy;Roman_Catholic_Church;HIV;R...  \n",
      "...                                                  ...  \n",
      "51313                   Yagan;Ancient_Egypt;Civilization  \n",
      "51314  Yagan;Folklore;Brothers_Grimm;Folklore;19th_ce...  \n",
      "51315  Yagan;Australia;England;France;United_States;T...  \n",
      "51316  Yarralumla,_Australian_Capital_Territory;Austr...  \n",
      "51317                            Ziad_Jarrah;Germany;Jew  \n",
      "\n",
      "[51318 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# Define the function again if necessary\n",
    "def replace_back_steps(path_str):\n",
    "    articles = path_str.split(';')\n",
    "    stack = []\n",
    "    processed = []\n",
    "    \n",
    "    for article in articles:\n",
    "        if article == '<':\n",
    "            if stack:\n",
    "                stack.pop()\n",
    "                if stack:\n",
    "                    last_article = stack[-1]\n",
    "                    processed.append(last_article)\n",
    "            # Else, skip appending anything\n",
    "        else:\n",
    "            stack.append(article)\n",
    "            processed.append(article)\n",
    "    \n",
    "    return ';'.join(processed)\n",
    "\n",
    "# Function to apply in parallel\n",
    "def parallel_process(paths):\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        processed_paths = pool.map(replace_back_steps, paths)\n",
    "    return processed_paths\n",
    "\n",
    "# Apply parallel processing\n",
    "paths_df['processed_path'] = parallel_process(paths_df['path'].tolist())\n",
    "\n",
    "# Verify the result\n",
    "print(paths_df[['path', 'processed_path']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming `paths_df` and `article_data` are your existing DataFrames\n",
    "\n",
    "# Step 1: Precompute Lookup Dictionaries\n",
    "# Ensure 'linkTarget' is a list. If it's a string separated by a delimiter (e.g., ';'), split it accordingly.\n",
    "# article_data['linkTarget'] = article_data['linkTarget'].apply(lambda x: x.split(';') if isinstance(x, str) else [])\n",
    "article_text_dict = article_data.set_index('article')['plain_text'].to_dict()\n",
    "article_links_dict = article_data.set_index('article')['linkTarget'].to_dict()\n",
    "\n",
    "# Step 2: Collect Data in Lists\n",
    "dataset_list = []\n",
    "\n",
    "for idx, row in paths_df.iterrows():\n",
    "    path_str = row['processed_path']\n",
    "    final_target_article = row['target_article']\n",
    "    \n",
    "    # Split the path into individual articles\n",
    "    current_path = path_str.split(';')\n",
    "    path_length = len(current_path)\n",
    "    \n",
    "    # Iterate over each article in the current path\n",
    "    for i, article_name in enumerate(current_path):\n",
    "        # Retrieve the current article's text\n",
    "        current_article_text = article_text_dict.get(article_name, \"\")\n",
    "        \n",
    "        # Retrieve the list of possible next articles\n",
    "        possible_next_articles = article_links_dict.get(article_name, [])\n",
    "        \n",
    "        # Determine the target article\n",
    "        if i + 1 < path_length:\n",
    "            target_article = current_path[i + 1]\n",
    "        else:\n",
    "            target_article = final_target_article\n",
    "        \n",
    "\n",
    "        # Append the data point to the list\n",
    "        dataset_list.append({\n",
    "            'current_article_text': current_article_text,\n",
    "            'possible_next_articles': possible_next_articles,\n",
    "            'target_article': target_article,\n",
    "            'final_target_article': final_target_article\n",
    "        })\n",
    "\n",
    "# Step 3: Bulk DataFrame Creation\n",
    "dataset = pd.DataFrame(dataset_list)\n",
    "\n",
    "# Optional: Free up memory by deleting the list\n",
    "del dataset_list\n",
    "\n",
    "# Optional: Reset index if needed\n",
    "dataset.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['current_article_text', 'possible_next_articles', 'target_article',\n",
       "       'final_target_article'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign unique IDs to articles\n",
    "\n",
    "all_articles = article_data['article'].tolist()\n",
    "\n",
    "article_to_id = {article: idx for idx, article in enumerate(sorted(all_articles))}\n",
    "id_to_article = {idx: article for article, idx in article_to_id.items()}\n",
    "id_dataset = pd.DataFrame()\n",
    "id_dataset[\"possible_next_ids\"] = dataset[\"possible_next_articles\"].apply(lambda x: [article_to_id[article] for article in x])\n",
    "id_dataset[\"target_id\"] = dataset[\"target_article\"].map(article_to_id)\n",
    "id_dataset[\"final_target_id\"] = dataset[\"final_target_article\"].map(article_to_id)\n",
    "id_dataset[\"current_article_text\"] = dataset[\"current_article_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>possible_next_ids</th>\n",
       "      <th>target_id</th>\n",
       "      <th>final_target_id</th>\n",
       "      <th>current_article_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[3, 6, 73, 417, 597, 714, 886, 899, 1128, 1131...</td>\n",
       "      <td>6</td>\n",
       "      <td>137</td>\n",
       "      <td>#copyright\\n\\n14th century\\n\\n2007 Schools ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 1, 2, 3, 4, 9, 11, 13, 18, 20, 21, 32, 33,...</td>\n",
       "      <td>9</td>\n",
       "      <td>137</td>\n",
       "      <td>#copyright\\n\\n15th century\\n\\n2007 Schools ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 1, 2, 3, 4, 6, 11, 13, 18, 20, 21, 32, 33,...</td>\n",
       "      <td>3127</td>\n",
       "      <td>137</td>\n",
       "      <td>#copyright\\n\\n16th century\\n\\n2007 Schools ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[9, 11, 13, 18, 82, 122, 277, 318, 321, 351, 3...</td>\n",
       "      <td>371</td>\n",
       "      <td>137</td>\n",
       "      <td>#copyright\\n\\nPacific Ocean\\n\\n2007 Schools...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[83, 87, 99, 122, 148, 160, 210, 226, 259, 277...</td>\n",
       "      <td>99</td>\n",
       "      <td>137</td>\n",
       "      <td>#copyright\\n\\nAtlantic Ocean\\n\\n2007 School...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   possible_next_ids  target_id  \\\n",
       "0  [3, 6, 73, 417, 597, 714, 886, 899, 1128, 1131...          6   \n",
       "1  [0, 1, 2, 3, 4, 9, 11, 13, 18, 20, 21, 32, 33,...          9   \n",
       "2  [0, 1, 2, 3, 4, 6, 11, 13, 18, 20, 21, 32, 33,...       3127   \n",
       "3  [9, 11, 13, 18, 82, 122, 277, 318, 321, 351, 3...        371   \n",
       "4  [83, 87, 99, 122, 148, 160, 210, 226, 259, 277...         99   \n",
       "\n",
       "   final_target_id                               current_article_text  \n",
       "0              137     #copyright\\n\\n14th century\\n\\n2007 Schools ...  \n",
       "1              137     #copyright\\n\\n15th century\\n\\n2007 Schools ...  \n",
       "2              137     #copyright\\n\\n16th century\\n\\n2007 Schools ...  \n",
       "3              137     #copyright\\n\\nPacific Ocean\\n\\n2007 Schools...  \n",
       "4              137     #copyright\\n\\nAtlantic Ocean\\n\\n2007 School...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = id_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   possible_next_ids  target_id  label\n",
      "0  [3, 6, 73, 417, 597, 714, 886, 899, 1128, 1131...          6      1\n",
      "1  [0, 1, 2, 3, 4, 9, 11, 13, 18, 20, 21, 32, 33,...          9      5\n",
      "2  [0, 1, 2, 3, 4, 6, 11, 13, 18, 20, 21, 32, 33,...       3127     70\n",
      "3  [9, 11, 13, 18, 82, 122, 277, 318, 321, 351, 3...        371     10\n",
      "4  [83, 87, 99, 122, 148, 160, 210, 226, 259, 277...         99      2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to find the index of target_article in possible_next_ids\n",
    "def find_target_index(possible_next_ids, target_id):\n",
    "    try:\n",
    "        return possible_next_ids.index(target_id)\n",
    "    except ValueError:\n",
    "        # If target_id not in possible_next_ids, return a default value, e.g., 0\n",
    "        return 0\n",
    "\n",
    "# Apply the function to create a 'label' column\n",
    "dataset['label'] = dataset.apply(\n",
    "    lambda row: find_target_index(row['possible_next_ids'], row['target_id']), axis=1\n",
    ")\n",
    "\n",
    "# Verify the labels\n",
    "print(dataset[['possible_next_ids', 'target_id', 'label']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_POSSIBLE_NEXT = 312  # Adjust based on your dataset\n",
    "\n",
    "# Pad possible_next_ids\n",
    "dataset['possible_next_ids_padded'] = pad_sequences(\n",
    "    dataset['possible_next_ids'],\n",
    "    maxlen=MAX_POSSIBLE_NEXT,\n",
    "    padding='post',\n",
    "    truncating='post',\n",
    "    value=0  # Assuming 0 is the padding ID\n",
    ").tolist()\n",
    "\n",
    "# If possible_next_ids are already padded during preprocessing, skip this step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>possible_next_ids</th>\n",
       "      <th>target_id</th>\n",
       "      <th>final_target_id</th>\n",
       "      <th>current_article_text</th>\n",
       "      <th>label</th>\n",
       "      <th>possible_next_ids_padded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[3, 6, 73, 417, 597, 714, 886, 899, 1128, 1131...</td>\n",
       "      <td>6</td>\n",
       "      <td>137</td>\n",
       "      <td>#copyright\\n\\n14th century\\n\\n2007 Schools ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 6, 73, 417, 597, 714, 886, 899, 1128, 1131...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 1, 2, 3, 4, 9, 11, 13, 18, 20, 21, 32, 33,...</td>\n",
       "      <td>9</td>\n",
       "      <td>137</td>\n",
       "      <td>#copyright\\n\\n15th century\\n\\n2007 Schools ...</td>\n",
       "      <td>5</td>\n",
       "      <td>[0, 1, 2, 3, 4, 9, 11, 13, 18, 20, 21, 32, 33,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 1, 2, 3, 4, 6, 11, 13, 18, 20, 21, 32, 33,...</td>\n",
       "      <td>3127</td>\n",
       "      <td>137</td>\n",
       "      <td>#copyright\\n\\n16th century\\n\\n2007 Schools ...</td>\n",
       "      <td>70</td>\n",
       "      <td>[0, 1, 2, 3, 4, 6, 11, 13, 18, 20, 21, 32, 33,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[9, 11, 13, 18, 82, 122, 277, 318, 321, 351, 3...</td>\n",
       "      <td>371</td>\n",
       "      <td>137</td>\n",
       "      <td>#copyright\\n\\nPacific Ocean\\n\\n2007 Schools...</td>\n",
       "      <td>10</td>\n",
       "      <td>[9, 11, 13, 18, 82, 122, 277, 318, 321, 351, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[83, 87, 99, 122, 148, 160, 210, 226, 259, 277...</td>\n",
       "      <td>99</td>\n",
       "      <td>137</td>\n",
       "      <td>#copyright\\n\\nAtlantic Ocean\\n\\n2007 School...</td>\n",
       "      <td>2</td>\n",
       "      <td>[83, 87, 99, 122, 148, 160, 210, 226, 259, 277...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   possible_next_ids  target_id  \\\n",
       "0  [3, 6, 73, 417, 597, 714, 886, 899, 1128, 1131...          6   \n",
       "1  [0, 1, 2, 3, 4, 9, 11, 13, 18, 20, 21, 32, 33,...          9   \n",
       "2  [0, 1, 2, 3, 4, 6, 11, 13, 18, 20, 21, 32, 33,...       3127   \n",
       "3  [9, 11, 13, 18, 82, 122, 277, 318, 321, 351, 3...        371   \n",
       "4  [83, 87, 99, 122, 148, 160, 210, 226, 259, 277...         99   \n",
       "\n",
       "   final_target_id                               current_article_text  label  \\\n",
       "0              137     #copyright\\n\\n14th century\\n\\n2007 Schools ...      1   \n",
       "1              137     #copyright\\n\\n15th century\\n\\n2007 Schools ...      5   \n",
       "2              137     #copyright\\n\\n16th century\\n\\n2007 Schools ...     70   \n",
       "3              137     #copyright\\n\\nPacific Ocean\\n\\n2007 Schools...     10   \n",
       "4              137     #copyright\\n\\nAtlantic Ocean\\n\\n2007 School...      2   \n",
       "\n",
       "                            possible_next_ids_padded  \n",
       "0  [3, 6, 73, 417, 597, 714, 886, 899, 1128, 1131...  \n",
       "1  [0, 1, 2, 3, 4, 9, 11, 13, 18, 20, 21, 32, 33,...  \n",
       "2  [0, 1, 2, 3, 4, 6, 11, 13, 18, 20, 21, 32, 33,...  \n",
       "3  [9, 11, 13, 18, 82, 122, 277, 318, 321, 351, 3...  \n",
       "4  [83, 87, 99, 122, 148, 160, 210, 226, 259, 277...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "def batch_preprocess_and_save_dataset(\n",
    "    dataset, \n",
    "    article_to_id, \n",
    "    tokenizer, \n",
    "    output_dir, \n",
    "    max_length=512, \n",
    "    max_possible_next=10, \n",
    "    batch_size=1000\n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset in batches and save to disk incrementally.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset (pd.DataFrame): DataFrame containing the data points.\n",
    "    - article_to_id (dict): Dictionary mapping article names to IDs.\n",
    "    - tokenizer (transformers.PreTrainedTokenizerFast): Fast tokenizer instance.\n",
    "    - output_dir (str): Directory to save processed batches.\n",
    "    - max_length (int): Maximum length for tokenization.\n",
    "    - max_possible_next (int): Maximum number of possible next articles.\n",
    "    - batch_size (int): Number of samples to process in each batch.\n",
    "    \"\"\"\n",
    "    num_samples = len(dataset)\n",
    "    batch_num = 0\n",
    "\n",
    "    # Iterate over the dataset in batches with a progress bar\n",
    "    for start in tqdm(range(0, num_samples, batch_size), desc='Processing Batches'):\n",
    "        end = min(start + batch_size, num_samples)\n",
    "        batch = dataset.iloc[start:end]\n",
    "\n",
    "        # 1. Tokenize 'current_article_text'\n",
    "        texts = batch['current_article_text'].tolist()\n",
    "        encoded = tokenizer(\n",
    "            texts,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='np'  # Return as NumPy arrays\n",
    "        )\n",
    "        input_ids_batch = encoded['input_ids']\n",
    "        attention_mask_batch = encoded['attention_mask']\n",
    "\n",
    "        # 2. Map 'possible_next_articles' to IDs with padding/truncation\n",
    "        # possible_next_articles = batch['possible_next_article'].tolist()\n",
    "        # possible_next_ids_batch = np.array([\n",
    "        #     [article_to_id.get(article, 0) for article in articles[:max_possible_next]] + [0]*(max_possible_next - len(articles)) \n",
    "        #     if len(articles) < max_possible_next else [article_to_id.get(article, 0) for article in articles[:max_possible_next]]\n",
    "        #     for articles in possible_next_articles\n",
    "        # ], dtype=np.int32)\n",
    "\n",
    "        posisble_next_ids_batch = np.array(batch['possible_next_ids_padded'].tolist(), dtype=np.int32)\n",
    "\n",
    "        # 3. Map 'target_article' and 'final_target_article' to IDs\n",
    "        # target_ids_batch = batch['target_article'].map(lambda x: article_to_id.get(x, 0)).values\n",
    "        # final_target_ids_batch = batch['final_target_article'].map(lambda x: article_to_id.get(x, 0)).values\n",
    "\n",
    "\n",
    "        target_ids_batch = batch['target_id'].values\n",
    "        final_target_ids_batch = batch['final_target_id'].values\n",
    "\n",
    "        # Combine into a dictionary\n",
    "        processed_batch = {\n",
    "            'input_ids': input_ids_batch,\n",
    "            'attention_mask': attention_mask_batch,\n",
    "            'possible_next_ids': posisble_next_ids_batch,\n",
    "            'target_id': np.array(target_ids_batch, dtype=np.int32),\n",
    "            'final_target_id': np.array(final_target_ids_batch, dtype=np.int32),\n",
    "        }\n",
    "\n",
    "        # Save batch to disk using pickle\n",
    "        batch_path = f\"{output_dir}/batch_{batch_num}.pkl\"\n",
    "        with open(batch_path, 'wb') as f:\n",
    "            pickle.dump(processed_batch, f)\n",
    "\n",
    "        batch_num += 1\n",
    "\n",
    "    print(f\"All batches saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DistilBertTokenizerFast\n",
    "\n",
    "# # Use the fast tokenizer\n",
    "# tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# batch_preprocess_and_save_dataset(\n",
    "#     dataset=dataset,\n",
    "#     article_to_id=article_to_id,\n",
    "#     tokenizer=tokenizer,\n",
    "#     output_dir='./processed_batches_ids_312',\n",
    "#     max_length=512,\n",
    "#     max_possible_next=312,\n",
    "#     batch_size=1000\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1734079187.974169   55036 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5520 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def load_sampled_batches_to_tf_dataset(batch_dir, sample_size=5):\n",
    "    \"\"\"\n",
    "    Load a random sample of batches from disk and create a TensorFlow dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - batch_dir (str): Directory containing the saved batches.\n",
    "    - sample_size (int): Number of batch files to sample.\n",
    "\n",
    "    Returns:\n",
    "    - tf.data.Dataset: TensorFlow dataset containing sampled data.\n",
    "    \"\"\"\n",
    "    # List all batch files in the directory\n",
    "    all_files = [f for f in sorted(os.listdir(batch_dir)) if f.endswith('.pkl')]\n",
    "\n",
    "    # Sample a subset of the files\n",
    "    sampled_files = random.sample(all_files, min(sample_size, len(all_files)))\n",
    "\n",
    "    # Placeholder lists for TensorFlow dataset creation\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    possible_next_ids_list = []\n",
    "    final_target_ids_list = []\n",
    "    target_ids_list = []\n",
    "\n",
    "    # Load sampled batches\n",
    "    for filename in sampled_files:\n",
    "        with open(os.path.join(batch_dir, filename), 'rb') as f:\n",
    "            batch = pickle.load(f)\n",
    "            \n",
    "            # Append to lists\n",
    "            input_ids_list.append(batch['input_ids'])\n",
    "            attention_mask_list.append(batch['attention_mask'])\n",
    "            possible_next_ids_list.append(batch['possible_next_ids'])\n",
    "            final_target_ids_list.append(batch['final_target_id'])\n",
    "            target_ids_list.append(batch['target_id'])\n",
    "\n",
    "    # Concatenate sampled batches into NumPy arrays\n",
    "    input_ids = np.concatenate(input_ids_list, axis=0)\n",
    "    attention_mask = np.concatenate(attention_mask_list, axis=0)\n",
    "    possible_next_ids = np.concatenate(possible_next_ids_list, axis=0)\n",
    "    final_target_ids = np.concatenate(final_target_ids_list, axis=0)\n",
    "    target_ids = np.concatenate(target_ids_list, axis=0)\n",
    "\n",
    "    # Reshape final_target_ids to (batch_size, 1)\n",
    "    final_target_ids = final_target_ids.reshape(-1, 1)\n",
    "    target_ids = target_ids.reshape(-1, 1)\n",
    "\n",
    "    # Create a TensorFlow dataset\n",
    "    tf_dataset = tf.data.Dataset.from_tensor_slices(({\n",
    "        'input_ids': tf.constant(input_ids, dtype=tf.int32),\n",
    "        'attention_mask': tf.constant(attention_mask, dtype=tf.int32),\n",
    "        'possible_next_ids': tf.constant(possible_next_ids, dtype=tf.int32),\n",
    "        'final_target_id': tf.constant(final_target_ids, dtype=tf.int32)\n",
    "    }, tf.constant(target_ids, dtype=tf.int32)))\n",
    "\n",
    "    return tf_dataset\n",
    "\n",
    "# Load a sampled dataset and apply transformations\n",
    "batch_dir = './processed_batches_ids_312'\n",
    "sample_size = 5  # Adjust based on your memory capacity\n",
    "\n",
    "tf_dataset = load_sampled_batches_to_tf_dataset(batch_dir, sample_size=sample_size)\n",
    "\n",
    "# Shuffle, batch, and prefetch\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "tf_dataset = tf_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': TensorSpec(shape=(None, 512), dtype=tf.int32, name=None),\n",
       "  'attention_mask': TensorSpec(shape=(None, 512), dtype=tf.int32, name=None),\n",
       "  'possible_next_ids': TensorSpec(shape=(None, 312), dtype=tf.int32, name=None),\n",
       "  'final_target_id': TensorSpec(shape=(None, 1), dtype=tf.int32, name=None)},\n",
       " TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_dataset.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to teach a model how to play the wikipedia game, one article and \"final target\" article at a time. I have a batched dataset that looks like the following:\n",
    "({'input_ids': TensorSpec(shape=(None, 512), dtype=tf.int32, name=None),\n",
    "  'attention_mask': TensorSpec(shape=(None, 512), dtype=tf.int32, name=None),\n",
    "  'possible_next_ids': TensorSpec(shape=(None, 32), dtype=tf.int32, name=None),\n",
    "  'final_target_id': TensorSpec(shape=(None, 1), dtype=tf.int32, name=None)},\n",
    " TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))\n",
    "\n",
    "Where \"input_ids\" are the first 512 tokens from the article text by distil_bert_uncased, and attention_mask is the mask output from calling the encoder, like so\n",
    "\n",
    "        # 1. Tokenize 'current_article_text'\n",
    "        texts = batch['current_article_text'].tolist()\n",
    "        encoded = tokenizer(\n",
    "            texts,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='np'  # Return as NumPy arrays\n",
    "        )\n",
    "        input_ids_batch = encoded['input_ids']\n",
    "        attention_mask_batch = encoded['attention_mask']\n",
    "\n",
    "possible_next_id's are the possible output articles of the current article, in numeric id form. these are the articles the model can choose from when outputting a prediciton for the next article to go to.\n",
    "final_target_id's are not the output of the model, but rather the final goal of the current round of playing the wikipeedia game. The model should be trying to get to this article as fast as possible.\n",
    "\n",
    "Finaly, the second term is the \"correct\" choice, or rather what a human chose as the next article in a successful round of playing the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': TensorShape([32, 512]), 'attention_mask': TensorShape([32, 512]), 'possible_next_ids': TensorShape([32, 312]), 'final_target_id': TensorShape([32, 1])}\n",
      "Targets shape: (32, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-13 09:39:48.398515: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for batch in tf_dataset.take(1):\n",
    "    inputs, targets = batch\n",
    "    print({k: v.shape for k, v in inputs.items()})\n",
    "    print(f\"Targets shape: {targets.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(32, 512), dtype=int32, numpy=\n",
      "array([[ 101, 1001, 9385, ..., 1997, 1996,  102],\n",
      "       [ 101, 1001, 9385, ..., 2001, 2511,  102],\n",
      "       [ 101, 1001, 9385, ..., 2166, 1040,  102],\n",
      "       ...,\n",
      "       [ 101, 1001, 9385, ..., 1998, 8192,  102],\n",
      "       [ 101, 1001, 9385, ..., 4100, 6882,  102],\n",
      "       [ 101, 1001, 9385, ..., 2167, 2712,  102]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(32, 512), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1]], dtype=int32)>, 'possible_next_ids': <tf.Tensor: shape=(32, 312), dtype=int32, numpy=\n",
      "array([[ 227,  938, 1561, ...,    0,    0,    0],\n",
      "       [ 140,  142,  159, ...,    0,    0,    0],\n",
      "       [ 168,  203,  223, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [   3,    4,    6, ...,    0,    0,    0],\n",
      "       [  98,  118,  351, ...,    0,    0,    0],\n",
      "       [  13,  122,  159, ...,    0,    0,    0]], dtype=int32)>, 'final_target_id': <tf.Tensor: shape=(32, 1), dtype=int32, numpy=\n",
      "array([[2112],\n",
      "       [1383],\n",
      "       [3410],\n",
      "       [3439],\n",
      "       [ 627],\n",
      "       [ 743],\n",
      "       [2782],\n",
      "       [2744],\n",
      "       [4506],\n",
      "       [4437],\n",
      "       [1383],\n",
      "       [2534],\n",
      "       [ 963],\n",
      "       [ 694],\n",
      "       [1614],\n",
      "       [1403],\n",
      "       [2542],\n",
      "       [ 390],\n",
      "       [3268],\n",
      "       [ 764],\n",
      "       [3135],\n",
      "       [ 139],\n",
      "       [ 602],\n",
      "       [2282],\n",
      "       [ 678],\n",
      "       [1547],\n",
      "       [3341],\n",
      "       [4456],\n",
      "       [ 186],\n",
      "       [2330],\n",
      "       [1724],\n",
      "       [2169]], dtype=int32)>}\n",
      "tf.Tensor(\n",
      "[[3187]\n",
      " [1426]\n",
      " [4291]\n",
      " [3439]\n",
      " [4032]\n",
      " [1490]\n",
      " [4291]\n",
      " [1341]\n",
      " [3532]\n",
      " [3876]\n",
      " [2018]\n",
      " [4287]\n",
      " [2893]\n",
      " [ 456]\n",
      " [1614]\n",
      " [2589]\n",
      " [1595]\n",
      " [ 688]\n",
      " [4374]\n",
      " [1766]\n",
      " [1379]\n",
      " [ 139]\n",
      " [ 621]\n",
      " [ 188]\n",
      " [3819]\n",
      " [1547]\n",
      " [3341]\n",
      " [ 321]\n",
      " [ 899]\n",
      " [2321]\n",
      " [3970]\n",
      " [2177]], shape=(32, 1), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-13 09:39:48.526127: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for batch in tf_dataset.take(1):\n",
    "\n",
    "    print(batch[0])\n",
    "    print(batch[1])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': TensorSpec(shape=(None, 512), dtype=tf.int32, name=None),\n",
       "  'attention_mask': TensorSpec(shape=(None, 512), dtype=tf.int32, name=None),\n",
       "  'possible_next_ids': TensorSpec(shape=(None, 312), dtype=tf.int32, name=None),\n",
       "  'final_target_id': TensorSpec(shape=(None, 1), dtype=tf.int32, name=None)},\n",
       " TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describe the dataset\n",
    "tf_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(32, 512), dtype=int32, numpy=\n",
      "array([[  101,  1001,  9385, ...,  6679,  1010,   102],\n",
      "       [  101,  1001,  9385, ...,  1012,  5460,   102],\n",
      "       [  101,  1001,  9385, ...,  2018,  2025,   102],\n",
      "       ...,\n",
      "       [  101,  1001,  9385, ...,  1011,  1015,   102],\n",
      "       [  101,  1001,  9385, ..., 22925,  1007,   102],\n",
      "       [  101,  1001,  9385, ...,  4100,  6882,   102]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(32, 512), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1]], dtype=int32)>, 'possible_next_ids': <tf.Tensor: shape=(32, 312), dtype=int32, numpy=\n",
      "array([[261, 387, 804, ...,   0,   0,   0],\n",
      "       [223, 302, 792, ...,   0,   0,   0],\n",
      "       [ 32, 113, 325, ...,   0,   0,   0],\n",
      "       ...,\n",
      "       [ 13, 212, 375, ...,   0,   0,   0],\n",
      "       [330, 389, 431, ...,   0,   0,   0],\n",
      "       [ 98, 118, 351, ...,   0,   0,   0]], dtype=int32)>, 'final_target_id': <tf.Tensor: shape=(32, 1), dtype=int32, numpy=\n",
      "array([[2542],\n",
      "       [ 276],\n",
      "       [4034],\n",
      "       [2869],\n",
      "       [4498],\n",
      "       [3861],\n",
      "       [ 678],\n",
      "       [ 969],\n",
      "       [2405],\n",
      "       [ 694],\n",
      "       [ 318],\n",
      "       [1383],\n",
      "       [1724],\n",
      "       [  60],\n",
      "       [4498],\n",
      "       [1151],\n",
      "       [4008],\n",
      "       [4506],\n",
      "       [1383],\n",
      "       [3876],\n",
      "       [1383],\n",
      "       [1007],\n",
      "       [1933],\n",
      "       [ 784],\n",
      "       [3358],\n",
      "       [ 803],\n",
      "       [4332],\n",
      "       [3335],\n",
      "       [ 770],\n",
      "       [1383],\n",
      "       [ 276],\n",
      "       [1724]], dtype=int32)>}\n",
      "input_ids: (32, 512)\n",
      "attention_mask: (32, 512)\n",
      "possible_next_ids: (32, 312)\n",
      "final_target_id: (32, 1)\n",
      "Targets: (32, 1)\n"
     ]
    }
   ],
   "source": [
    "for batch in tf_dataset.take(1):\n",
    "    inputs, targets = batch\n",
    "    print(inputs)\n",
    "    for key, value in inputs.items():\n",
    "        print(f\"{key}: {value.shape}\")\n",
    "    print(f\"Targets: {targets.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def id_to_index(features, label_id):\n",
    "    # label_id is of shape (batch_size, 1), we need to squeeze it\n",
    "    label_id = tf.squeeze(label_id, axis=1)  # Now shape (batch_size,)\n",
    "\n",
    "    possible_ids = features['possible_next_ids']  # shape: (batch_size, 32)\n",
    "\n",
    "    # Create a boolean mask for each position: True where possible_ids == label_id\n",
    "    matches = tf.equal(possible_ids, tf.expand_dims(label_id, axis=1))  # (batch_size, 32)\n",
    "\n",
    "    # Convert boolean to int to use argmax\n",
    "    matches_int = tf.cast(matches, tf.int32)\n",
    "\n",
    "    # If the label_id is guaranteed to be in possible_next_ids, argmax will find the correct index.\n",
    "    # argmax on a row of all zeros would yield 0, so ensure that there's always a match.\n",
    "    index = tf.argmax(matches_int, axis=1)  # (batch_size,)\n",
    "\n",
    "    # index is now the integer index of the label_id in possible_next_ids.\n",
    "    return features, index\n",
    "\n",
    "\n",
    "# Apply this transformation to your dataset:\n",
    "tf_dataset_indexed = tf_dataset.map(id_to_index)\n",
    "\n",
    "# Now tf_dataset_indexed should yield ((features), label_index) \n",
    "# where label_index is in [0, num_candidates-1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(32, 512), dtype=int32, numpy=\n",
      "array([[ 101, 1001, 9385, ..., 1012, 2010,  102],\n",
      "       [ 101, 1001, 9385, ..., 2012, 2560,  102],\n",
      "       [ 101, 1001, 9385, ..., 2022, 4149,  102],\n",
      "       ...,\n",
      "       [ 101, 1001, 9385, ..., 1996, 2349,  102],\n",
      "       [ 101, 1001, 9385, ..., 1019, 1081,  102],\n",
      "       [ 101, 1001, 9385, ..., 1996, 2826,  102]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(32, 512), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1]], dtype=int32)>, 'possible_next_ids': <tf.Tensor: shape=(32, 312), dtype=int32, numpy=\n",
      "array([[171, 394, 465, ...,   0,   0,   0],\n",
      "       [ 16,  31, 517, ...,   0,   0,   0],\n",
      "       [281, 394, 465, ...,   0,   0,   0],\n",
      "       ...,\n",
      "       [ 13,  21,  56, ...,   0,   0,   0],\n",
      "       [122, 224, 265, ...,   0,   0,   0],\n",
      "       [122, 143, 239, ...,   0,   0,   0]], dtype=int32)>, 'final_target_id': <tf.Tensor: shape=(32, 1), dtype=int32, numpy=\n",
      "array([[4034],\n",
      "       [2591],\n",
      "       [2535],\n",
      "       [ 876],\n",
      "       [4450],\n",
      "       [2063],\n",
      "       [ 145],\n",
      "       [2103],\n",
      "       [2579],\n",
      "       [ 487],\n",
      "       [3042],\n",
      "       [4456],\n",
      "       [  53],\n",
      "       [2140],\n",
      "       [1616],\n",
      "       [1771],\n",
      "       [1151],\n",
      "       [4580],\n",
      "       [2112],\n",
      "       [1403],\n",
      "       [1797],\n",
      "       [3353],\n",
      "       [ 292],\n",
      "       [4234],\n",
      "       [ 380],\n",
      "       [ 952],\n",
      "       [ 292],\n",
      "       [2869],\n",
      "       [1872],\n",
      "       [1383],\n",
      "       [ 292],\n",
      "       [1383]], dtype=int32)>}\n",
      "Label Index: [ 17   4  15   0   0  14   1  29  17  53   0 124  13 128   9  62 131   0\n",
      "   0  15  91  55  48   0   0  37   0  39  17   6  32  14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-13 09:39:49.496978: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for batch in tf_dataset_indexed.take(1):\n",
    "    inputs, label_index = batch\n",
    "    print(inputs)\n",
    "    print(f\"Label Index: {label_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFDistilBertModel\n",
    "\n",
    "# Load DistilBERT\n",
    "distilbert = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example hyperparameters\n",
    "embedding_dim = 64\n",
    "max_length = 512\n",
    "num_candidates = 312\n",
    "num_article_ids = 500000  # Adjust based on your dataset\n",
    "\n",
    "class WikiNextArticleModel(tf.keras.Model):\n",
    "    def __init__(self, distilbert, embedding_dim, num_article_ids, num_candidates):\n",
    "        super().__init__()\n",
    "        self.distilbert = distilbert\n",
    "        self.final_target_embedding = tf.keras.layers.Embedding(num_article_ids, embedding_dim)\n",
    "        self.candidate_embedding = tf.keras.layers.Embedding(num_article_ids, embedding_dim)\n",
    "        self.query_dense_1 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.query_dense_2 = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.softmax = tf.keras.layers.Softmax()\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        # inputs is a dict: {'input_ids':..., 'attention_mask':..., 'possible_next_ids':..., 'final_target_id':...}\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        possible_next_ids = inputs['possible_next_ids']\n",
    "        final_target_id = inputs['final_target_id']\n",
    "        \n",
    "        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask, training=training)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        cls_rep = sequence_output[:, 0, :]\n",
    "\n",
    "        final_target_emb = tf.squeeze(self.final_target_embedding(final_target_id), axis=1)\n",
    "        candidates_emb = self.candidate_embedding(possible_next_ids)\n",
    "        \n",
    "        combined_rep = tf.concat([cls_rep, final_target_emb], axis=-1)\n",
    "        query = self.query_dense_1(combined_rep)\n",
    "        query = self.query_dense_2(query)\n",
    "\n",
    "        query_expanded = tf.expand_dims(query, axis=1)\n",
    "        logits = tf.reduce_sum(query_expanded * candidates_emb, axis=-1)\n",
    "        return self.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example validation:\n",
    "# # extract labels from a batch\n",
    "# for batch in tf_dataset_indexed.take(1):\n",
    "#     inputs, targets = batch\n",
    "#     labels = targets\n",
    "#     tf.debugging.assert_less(labels, num_candidates, message=\"Labels out of candidate range\")\n",
    "#     tf.debugging.assert_greater_equal(labels, 0, message=\"Labels contain negative indices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': (TensorShape([32, 512]), tf.int32, <tf.Tensor: shape=(), dtype=int32, numpy=100>, <tf.Tensor: shape=(), dtype=int32, numpy=29837>), 'attention_mask': (TensorShape([32, 512]), tf.int32, <tf.Tensor: shape=(), dtype=int32, numpy=1>, <tf.Tensor: shape=(), dtype=int32, numpy=1>), 'possible_next_ids': (TensorShape([32, 312]), tf.int32, <tf.Tensor: shape=(), dtype=int32, numpy=0>, <tf.Tensor: shape=(), dtype=int32, numpy=4596>), 'final_target_id': (TensorShape([32, 1]), tf.int32, <tf.Tensor: shape=(), dtype=int32, numpy=53>, <tf.Tensor: shape=(), dtype=int32, numpy=4506>)}\n",
      "Label:  (32,) <dtype: 'int64'> tf.Tensor(0, shape=(), dtype=int64) tf.Tensor(88, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for features, label in tf_dataset_indexed.take(1):\n",
    "    print({k: (v.shape, v.dtype, tf.reduce_min(v), tf.reduce_max(v)) for k,v in features.items()})\n",
    "    print(\"Label: \", label.shape, label.dtype, tf.reduce_min(label), tf.reduce_max(label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1734079204.184557   55450 service.cc:148] XLA service 0x7fd90c003b30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1734079204.184920   55450 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9\n",
      "2024-12-13 09:40:04.382949: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "W0000 00:00:1734079204.448089   55450 assert_op.cc:38] Ignoring Assert operator wiki_next_article_model_1/tf_distil_bert_model/distilbert/embeddings/assert_less/Assert/Assert\n",
      "2024-12-13 09:40:04.507504: W tensorflow/compiler/tf2xla/kernels/random_ops.cc:61] Warning: Using tf.random.uniform with XLA compilation will ignore seeds; consider using tf.random.stateless_uniform instead if reproducible behavior is desired. wiki_next_article_model_1/tf_distil_bert_model/distilbert/embeddings/dropout/dropout/random_uniform/RandomUniform\n",
      "I0000 00:00:1734079205.211818   55450 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2024-12-13 09:40:06.908891: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10077', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2024-12-13 09:40:06.981052: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10077', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2024-12-13 09:40:07.128309: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10077', 40 bytes spill stores, 40 bytes spill loads\n",
      "\n",
      "I0000 00:00:1734079209.910798   55450 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m156/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 310ms/step - accuracy: 0.0637 - loss: 4.6386"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1734079258.603544   55450 assert_op.cc:38] Ignoring Assert operator wiki_next_article_model_1/tf_distil_bert_model/distilbert/embeddings/assert_less/Assert/Assert\n",
      "2024-12-13 09:41:00.597724: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2024-12-13 09:41:00.756338: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2024-12-13 09:41:00.778827: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2024-12-13 09:41:00.946015: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1', 288 bytes spill stores, 288 bytes spill loads\n",
      "\n",
      "2024-12-13 09:41:00.994694: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1', 344 bytes spill stores, 316 bytes spill loads\n",
      "\n",
      "2024-12-13 09:41:01.171181: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 354ms/step - accuracy: 0.0636 - loss: 4.6322\n",
      "Epoch 2/3\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 330ms/step - accuracy: 0.0733 - loss: 3.6803\n",
      "Epoch 3/3\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 317ms/step - accuracy: 0.0896 - loss: 3.5778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fd8e6907e90>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Instantiate the model\n",
    "model = WikiNextArticleModel(\n",
    "    distilbert=distilbert,\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_article_ids=num_article_ids,\n",
    "    num_candidates=num_candidates\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# tf_dataset should yield (({'input_ids': ..., 'attention_mask': ..., 'possible_next_ids': ..., 'final_target_id': ...}), label)\n",
    "model.fit(tf_dataset_indexed, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (32, 312)\n",
      "Predictions: [[4.4862136e-02 1.9876620e-02 1.3106190e-01 ... 4.0875716e-06\n",
      "  4.0875716e-06 4.0875716e-06]\n",
      " [3.1578451e-02 2.3546707e-02 1.3596018e-02 ... 6.4135878e-07\n",
      "  6.4135878e-07 6.4135878e-07]\n",
      " [1.3206375e-02 5.8479551e-03 7.7529019e-03 ... 4.8313024e-07\n",
      "  4.8313024e-07 4.8313024e-07]\n",
      " ...\n",
      " [8.8270027e-03 1.5991896e-02 1.1383730e-02 ... 4.3368175e-07\n",
      "  4.3368175e-07 4.3368175e-07]\n",
      " [1.0467061e-01 5.1200185e-02 1.6160339e-01 ... 2.9170822e-06\n",
      "  2.9170822e-06 2.9170822e-06]\n",
      " [1.4100142e-01 7.6302573e-02 2.0335525e-02 ... 1.3998890e-06\n",
      "  1.3998890e-06 1.3998890e-06]]\n",
      "Predicted Index: [  2  79 121  24  15   1  72  39  43   8 113   7  28  79   1  18   1  35\n",
      "   2 195  60  43  24  37   0   0  30  92   0  92   2  14]\n",
      "True Index: [ 6 48  9 33  0  2  0 37 22  3  8  0 23 46  4 16 48 10  0 82 60  0  4  0\n",
      " 52  0 20  8 11  9  2  0]\n"
     ]
    }
   ],
   "source": [
    "# Display predictions of model on a datapoint.\n",
    "\n",
    "for features, label in tf_dataset_indexed.take(1):\n",
    "    predictions = model(features)\n",
    "    print(f\"Predictions shape: {predictions.shape}\")\n",
    "    print(f\"Predictions: {predictions}\")\n",
    "    # Decode the predictions to get the predicted index\n",
    "    predicted_index = tf.argmax(predictions, axis=1)\n",
    "    print(f\"Predicted Index: {predicted_index}\")\n",
    "    print(f\"True Index: {label}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FDH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
