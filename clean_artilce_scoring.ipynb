{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions used to compute the different scores\n",
    "\n",
    "They can be moved to a util.py file later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "import pyarrow.feather as feather\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for utils later to get the average weights of articles from a DataFrame containing path information\n",
    "\n",
    "def calculate_avg_article_weights(df, count_cutoff=30, scaling=None):\n",
    "    \"\"\"\n",
    "    Calculate the average weights of articles from a DataFrame containing path information.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame with the following columns:\n",
    "            - 'simplified_path': List of articles in the path\n",
    "            - 'simplified_path_length': Length of the simplified path\n",
    "            - 'distance': Distance associated with the path\n",
    "        scaling (str): Type of scaling to use. Options are 'minmax', 'standard', and 'robust' or None\n",
    "        count_cutoff (int): Minimum number of appearances for an article to be considered\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing:\n",
    "            - 'article': Article name\n",
    "            - 'n_appearances': Number of times the article appeared in paths\n",
    "            - 'weighted_avg': Weighted average of distances for the article\n",
    "    \"\"\"\n",
    "    # Copy and preprocess the DataFrame\n",
    "    df = df[['simplified_path', 'simplified_path_length', 'distance']].copy()\n",
    "    df['simplified_path'] = df['simplified_path'].apply(lambda l: l[1:-1])  # Remove start and end articles\n",
    "\n",
    "    # Calculate weight for each path\n",
    "    df['weight'] = df['distance'] / df['simplified_path_length']\n",
    "\n",
    "    # Initialize an empty DataFrame to store results\n",
    "    avg_article_weight_df = pd.DataFrame(columns=['article', 'n_appearances', 'weighted_avg'])\n",
    "    avg_article_weight_df.set_index('article', inplace=True)\n",
    "\n",
    "    # Iterate through each row to calculate weights\n",
    "    for _, row in df.iterrows():\n",
    "        weight = row['weight']\n",
    "        simplified_path = row['simplified_path']\n",
    "\n",
    "        for article in simplified_path:\n",
    "            if article not in avg_article_weight_df.index:\n",
    "                avg_article_weight_df.loc[article] = [0, 0]\n",
    "\n",
    "            # Update counts and weighted sums\n",
    "            avg_article_weight_df.at[article, 'n_appearances'] += 1\n",
    "            avg_article_weight_df.at[article, 'weighted_avg'] += weight\n",
    "\n",
    "    # Calculate the weighted average by dividing weighted sum by counts\n",
    "    avg_article_weight_df['weighted_avg'] = avg_article_weight_df['weighted_avg'] / avg_article_weight_df['n_appearances']\n",
    "\n",
    "    # Filter out articles that appear less than the cutoff\n",
    "    avg_article_weight_df = avg_article_weight_df[avg_article_weight_df['n_appearances'] >= count_cutoff]\n",
    "\n",
    "    # Normalize the weighted average\n",
    "    if scaling is not None:\n",
    "\n",
    "        if scaling == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "        elif scaling == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        elif scaling == 'robust':\n",
    "            scaler = RobustScaler()\n",
    "\n",
    "        avg_article_weight_df[scaling] = scaler.fit_transform(avg_article_weight_df[['weighted_avg']])\n",
    "\n",
    "\n",
    "    print(f\"Number of unique articles after weighting: {avg_article_weight_df.shape[0]}\")\n",
    "\n",
    "    return avg_article_weight_df#.reset_index()\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "\n",
    "# function for utils later to get the average weights of articles from a DataFrame containing path information\n",
    "\n",
    "def calculate_sum_article_cweights(df, count_cutoff=30, scaling=None):\n",
    "    \"\"\"\n",
    "    Calculate the sum of the centered weights of articles from a DataFrame containing path information.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame with the following columns:\n",
    "            - 'simplified_path': List of articles in the path\n",
    "            - 'simplified_path_length': Length of the simplified path\n",
    "            - 'distance': Distance associated with the path\n",
    "        scaling (str): Type of scaling to use. Options are 'minmax', 'standard', and 'robust' or None\n",
    "        count_cutoff (int): Minimum number of appearances for an article to be considered\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing:\n",
    "            - 'article': Article name\n",
    "            - 'n_appearances': Number of times the article appeared in paths\n",
    "            - 'weighted_sum': sum of the centered path weights for the article\n",
    "    \"\"\"\n",
    "    # Copy and preprocess the DataFrame\n",
    "    df = df[['simplified_path', 'simplified_path_length', 'distance']].copy()\n",
    "    df['simplified_path'] = df['simplified_path'].apply(lambda l: l[1:-1])  # Remove start and end articles\n",
    "\n",
    "    # Calculate weight for each path\n",
    "    df['weight'] = df['distance'] / df['simplified_path_length']\n",
    "\n",
    "    # Calculate mean weight\n",
    "    article_mean_weight = (df['weight'] * (df['simplified_path_length']-1)).sum() / (df['simplified_path_length']-1).sum() # -1 beacuse we don't want to include the target article\n",
    "\n",
    "    # Center the weights by subtracting the mean\n",
    "    df['centered_weight'] = df['weight'] - article_mean_weight\n",
    "\n",
    "    # Initialize an empty DataFrame to store results\n",
    "    sum_article_cweight_df = pd.DataFrame(columns=['article', 'n_appearances', 'weighted_sum'])\n",
    "    sum_article_cweight_df.set_index('article', inplace=True)\n",
    "\n",
    "    # Iterate through each row to calculate weights\n",
    "    for _, row in df.iterrows():\n",
    "        cweight = row['centered_weight']\n",
    "        simplified_path = row['simplified_path']\n",
    "\n",
    "        for article in simplified_path:\n",
    "            if article not in sum_article_cweight_df.index:\n",
    "                sum_article_cweight_df.loc[article] = [0, 0]\n",
    "\n",
    "            # Update counts and weighted sums\n",
    "            sum_article_cweight_df.at[article, 'n_appearances'] += 1\n",
    "            sum_article_cweight_df.at[article, 'weighted_sum'] += cweight\n",
    "\n",
    "    # Filter out articles that appear less than the cutoff\n",
    "    sum_article_cweight_df = sum_article_cweight_df[sum_article_cweight_df['n_appearances'] >= count_cutoff]\n",
    "\n",
    "    # Normalize the weighted average\n",
    "    if scaling is not None:\n",
    "\n",
    "        if scaling == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "        elif scaling == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        elif scaling == 'robust':\n",
    "            scaler = RobustScaler()\n",
    "\n",
    "        sum_article_cweight_df[scaling] = scaler.fit_transform(sum_article_cweight_df[['weighted_sum']])\n",
    "\n",
    "\n",
    "    print(f\"Number of unique articles after weighting: {sum_article_cweight_df.shape[0]}\")\n",
    "\n",
    "    return sum_article_cweight_df\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "\n",
    "\n",
    "# code a function that returns the ratio of the number of times an article appears in unfinished paths over the total number of times it appears\n",
    "\n",
    "def calculate_unfinished_ratios(in_df, count_cutoff=30, scaling=None):\n",
    "    \"\"\"\n",
    "    Calculate the ratio of the number of times an article appears in unfinished paths over the total number of times it appears.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame with the following columns:\n",
    "            - 'simplified_path': List of articles in the path\n",
    "        count_cutoff (int): Minimum number of appearances for an article to be considered\n",
    "        scaling (str): Type of scaling to use. Options are 'minmax', 'standard', and 'robust' or None\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A Series containing the ratio for each article\n",
    "    \"\"\"\n",
    "    # Copy and preprocess the DataFrame\n",
    "    df = in_df[['simplified_path', 'finished']].copy()\n",
    "    df['simplified_path'] = df['simplified_path'].apply(lambda l: l[1:-1])  # Remove start and end articles\n",
    "\n",
    "    # Initialize a dictionary to store counts\n",
    "    article_counts = {}\n",
    "    unfinished_counts = {}\n",
    "\n",
    "    # Iterate through each row to calculate counts\n",
    "    for _, row in df.iterrows():\n",
    "        simplified_path = row['simplified_path']\n",
    "        finished = row['finished']\n",
    "\n",
    "        for article in simplified_path:\n",
    "            article_counts[article] = article_counts.get(article, 0) + 1\n",
    "        \n",
    "        if not finished:\n",
    "            for article in simplified_path:\n",
    "                unfinished_counts[article] = unfinished_counts.get(article, 0) + 1\n",
    "\n",
    "    # Convert the dictionary to a Series\n",
    "    article_counts = pd.Series(article_counts)\n",
    "    unfinished_counts = pd.Series(unfinished_counts)\n",
    "\n",
    "    ratio = unfinished_counts / article_counts\n",
    "\n",
    "    ratio_df = pd.DataFrame({\n",
    "    'n_appearances': article_counts,\n",
    "    'unfinished_counts': unfinished_counts,\n",
    "    'unfinished_ratio': ratio\n",
    "    }).fillna(0)\n",
    "\n",
    "    # cut off\n",
    "    ratio_df = ratio_df[ratio_df['n_appearances'] >= count_cutoff]\n",
    "\n",
    "    # scaling\n",
    "    if scaling is not None:\n",
    "        if scaling == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "        elif scaling == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        elif scaling == 'robust':\n",
    "            scaler = RobustScaler()\n",
    "        \n",
    "        ratio_df[scaling] = -scaler.fit_transform(ratio_df[['unfinished_ratio']])\n",
    "\n",
    "    #print(f\"Number of unique articles: {len(article_counts)}\")\n",
    "    print(f\"Ratio of unfinished over finished paths: {1-df['finished'].mean()}\")\n",
    "    return ratio_df\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "\n",
    "\n",
    "# code a function that counts the number of dead ends an article has (difference between full path list content and simplified path list content)\n",
    "\n",
    "def calculate_detour_ratios(in_df, count_cutoff=1, scaling=None):\n",
    "    \"\"\"\n",
    "    Calculate the detour ratio for articles based on the full path and simplified path.\n",
    "\n",
    "    Parameters:\n",
    "        in_df (pd.DataFrame): Input DataFrame with the following columns:\n",
    "            - 'full_path': List of articles in the full path\n",
    "            - 'simplified_path': List of articles in the simplified path\n",
    "        count_cutoff (int): Minimum number of detours for an article to be considered.\n",
    "        scaling (str): Type of scaling to use. Options are 'minmax', 'standard', and 'robust' or None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the detour ratio and scaled values for each article.\n",
    "    \"\"\"\n",
    "    # Copy and preprocess the DataFrame\n",
    "    df = in_df[['full_path', 'simplified_path']].copy()\n",
    "    df['simplified_path'] = df['simplified_path'].apply(lambda l: l[1:-1])  # Remove start and end articles\n",
    "    df['full_path'] = df['full_path'].apply(lambda l: l[1:-1])  # Remove start and end articles\n",
    "\n",
    "    # Initialize dictionaries to store counts\n",
    "    detour_counts = {}\n",
    "    total_counts = {}\n",
    "\n",
    "    # Iterate through each row to calculate detour counts and total appearances\n",
    "    for _, row in df.iterrows():\n",
    "        full_path = row['full_path']\n",
    "        simplified_path = row['simplified_path']\n",
    "\n",
    "        # Count total appearances for articles in the full path\n",
    "        for article in full_path:\n",
    "            total_counts[article] = total_counts.get(article, 0) + 1\n",
    "\n",
    "        # Find detour articles by subtracting the simplified path from the full path\n",
    "        detour_articles = set(full_path) - set(simplified_path)\n",
    "        for article in detour_articles:\n",
    "            detour_counts[article] = detour_counts.get(article, 0) + 1\n",
    "\n",
    "    # Convert counts to Series\n",
    "    detour_counts = pd.Series(detour_counts)\n",
    "    total_counts = pd.Series(total_counts)\n",
    "\n",
    "    # Fill missing detour counts with 0 for articles with no detours\n",
    "    detour_counts = detour_counts.reindex(total_counts.index, fill_value=0)\n",
    "\n",
    "    # Calculate detour ratio\n",
    "    detour_ratios = detour_counts / total_counts\n",
    "\n",
    "    # Create a DataFrame with detour counts and ratios\n",
    "    detour_df = pd.DataFrame({\n",
    "        'detour_count': detour_counts,\n",
    "        'total_count': total_counts,\n",
    "        'detour_ratio': detour_ratios\n",
    "    }).loc[detour_ratios.index]\n",
    "\n",
    "    # Filter out articles with detour ratio less than the count_cutoff\n",
    "    detour_df = detour_df[detour_df['total_count'] >= count_cutoff]\n",
    "\n",
    "    if scaling is not None:\n",
    "        # normalize\n",
    "        if scaling == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "        elif scaling == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        elif scaling == 'robust':\n",
    "            scaler = RobustScaler()\n",
    "\n",
    "        detour_df[scaling] = -scaler.fit_transform(detour_df[['detour_ratio']])\n",
    "\n",
    "    print(f\"Number of unique articles after detour ratio calculation: {len(detour_df)}\")\n",
    "\n",
    "    return detour_df\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "\n",
    "def calc_avg_article_time(df, count_cutoff=30, scaling=None):\n",
    "    \"\"\"\n",
    "    Calculate the average speed of articles from a DataFrame containing path information.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame with the following columns:\n",
    "            - 'simplified_path': List of articles in the path\n",
    "            - 'durationInSec': Duration associated with the path\n",
    "        count_cutoff (int): Minimum number of appearances for an article to be considered\n",
    "        scaling (str): Type of scaling to use. Options are 'minmax', 'standard', and 'robust' or None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing:\n",
    "            - 'article': Article name\n",
    "            - 'n_appearances': Number of times the article appeared in paths\n",
    "            - 'avg_speed': Average speed of the article\n",
    "    \"\"\"\n",
    "    # Copy and preprocess the DataFrame\n",
    "    df = df[['simplified_path', 'durationInSec']].copy()\n",
    "\n",
    "    df['simplified_path'] = df['simplified_path'].apply(lambda l: l[1:-1])  # Remove start and end articles\n",
    "\n",
    "    # Initialize an empty DataFrame to store results\n",
    "    avg_article_speed_df = pd.DataFrame(columns=['article', 'n_appearances', 'avg_speed'])\n",
    "    avg_article_speed_df.set_index('article', inplace=True)\n",
    "\n",
    "    # Iterate through each row to calculate speeds\n",
    "    for _, row in df.iterrows():\n",
    "        speed = row['durationInSec']\n",
    "        simplified_path = row['simplified_path']\n",
    "\n",
    "        for article in simplified_path:\n",
    "            if article not in avg_article_speed_df.index:\n",
    "                avg_article_speed_df.loc[article] = [0, 0]\n",
    "\n",
    "            # Update counts and sums\n",
    "            avg_article_speed_df.at[article, 'n_appearances'] += 1\n",
    "            avg_article_speed_df.at[article, 'avg_speed'] += speed\n",
    "\n",
    "    # Calculate the average speed by dividing sum by counts\n",
    "    avg_article_speed_df['avg_speed'] = avg_article_speed_df['avg_speed'] / avg_article_speed_df['n_appearances']\n",
    "\n",
    "    # Filter out articles that appear less than the cutoff\n",
    "    avg_article_speed_df = avg_article_speed_df[avg_article_speed_df['n_appearances'] >= count_cutoff]\n",
    "\n",
    "    if scaling is not None:\n",
    "        # Normalize the average speed\n",
    "        if scaling == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "        elif scaling == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        elif scaling == 'robust':\n",
    "            scaler = RobustScaler()\n",
    "        \n",
    "        avg_article_speed_df[scaling] = -scaler.fit_transform(avg_article_speed_df[['avg_speed']])\n",
    "\n",
    "    print(f\"Number of unique articles after time calc: {avg_article_speed_df.shape[0]}\")\n",
    "\n",
    "    return avg_article_speed_df#.reset_index()\n",
    "\n",
    "\n",
    "# COMMENT: could consider really computing the speed instead of the duration. speed = distance / time and then sum up and average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_avg_article_speed(df, count_cutoff=30, scaling=None):\n",
    "    \"\"\"\n",
    "    Calculate the average speed of articles from a DataFrame containing path information.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame with the following columns:\n",
    "            - 'simplified_path': List of articles in the path\n",
    "            - 'durationInSec': Duration associated with the path\n",
    "            - 'full_path_length': Total length of the path\n",
    "        count_cutoff (int): Minimum number of appearances for an article to be considered.\n",
    "        scaling (str): Type of scaling to use. Options are 'minmax', 'standard', and 'robust' or None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing:\n",
    "            - 'article': Article name.\n",
    "            - 'n_appearances': Number of times the article appeared in paths.\n",
    "            - 'avg_speed': Average speed of the article.\n",
    "    \"\"\"\n",
    "    # Copy and preprocess the DataFrame\n",
    "    df = df[['simplified_path', 'durationInSec', 'full_path_length']].copy()\n",
    "\n",
    "    # Calculate the speed for each path\n",
    "    df['speed'] = df['durationInSec'] / df['full_path_length']\n",
    "\n",
    "    # Remove the start and end articles from the simplified path\n",
    "    df['simplified_path'] = df['simplified_path'].apply(lambda l: l[1:-1])  # Adjust as per your input structure\n",
    "\n",
    "    # Initialize an empty DataFrame to store results\n",
    "    avg_article_speed_df = pd.DataFrame(columns=['article', 'n_appearances', 'avg_speed'])\n",
    "    avg_article_speed_df.set_index('article', inplace=True)\n",
    "\n",
    "    # Iterate through each row to calculate speeds\n",
    "    for _, row in df.iterrows():\n",
    "        speed = row['speed']\n",
    "        simplified_path = row['simplified_path']\n",
    "\n",
    "        for article in simplified_path:\n",
    "            if article not in avg_article_speed_df.index:\n",
    "                avg_article_speed_df.loc[article] = [0, 0]\n",
    "\n",
    "            # Update counts and sums\n",
    "            avg_article_speed_df.at[article, 'n_appearances'] += 1\n",
    "            avg_article_speed_df.at[article, 'avg_speed'] += speed\n",
    "\n",
    "    # Calculate the average speed by dividing sum by counts\n",
    "    avg_article_speed_df['avg_speed'] = avg_article_speed_df['avg_speed'] / avg_article_speed_df['n_appearances']\n",
    "\n",
    "    # Filter out articles that appear less than the cutoff\n",
    "    avg_article_speed_df = avg_article_speed_df[avg_article_speed_df['n_appearances'] >= count_cutoff]\n",
    "\n",
    "    if scaling is not None:\n",
    "        # Normalize the average speed\n",
    "        if scaling == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "        elif scaling == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        elif scaling == 'robust':\n",
    "            scaler = RobustScaler()\n",
    "        \n",
    "        avg_article_speed_df[scaling] = -scaler.fit_transform(avg_article_speed_df[['avg_speed']])\n",
    "\n",
    "    print(f\"Number of unique articles after speed calc: {avg_article_speed_df.shape[0]}\")\n",
    "\n",
    "    return avg_article_speed_df  # Return the updated DataFrame\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "\n",
    "def calc_sum_article_cspeed(df, count_cutoff=30, scaling=None):\n",
    "    \"\"\"\n",
    "    Calculate the sum of the centered speeds of articles from a DataFrame containing path information.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame with the following columns:\n",
    "            - 'simplified_path': List of articles in the path\n",
    "            - 'durationInSec': Duration associated with the path\n",
    "            - 'full_path_length': Total length of the path\n",
    "        count_cutoff (int): Minimum number of appearances for an article to be considered.\n",
    "        scaling (str): Type of scaling to use. Options are 'minmax', 'standard', and 'robust' or None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing:\n",
    "            - 'article': Article name.\n",
    "            - 'n_appearances': Number of times the article appeared in paths.\n",
    "            - 'sum_cspeed': Sum of the centered path speeds for the article.\n",
    "    \"\"\"\n",
    "    # Copy and preprocess the DataFrame\n",
    "    df = df[['simplified_path', 'durationInSec', 'full_path_length']].copy()\n",
    "\n",
    "    # Calculate the speed for each path\n",
    "    df['speed'] = df['durationInSec'] / df['full_path_length']\n",
    "\n",
    "    # Calculate the mean speed\n",
    "    article_mean_speed = (df['speed'] * df['full_path_length']).sum() / df['full_path_length'].sum()\n",
    "\n",
    "    # Center the speeds by subtracting the mean\n",
    "    df['centered_speed'] = df['speed'] - article_mean_speed\n",
    "\n",
    "    # Remove the start and end articles from the simplified path\n",
    "    df['simplified_path'] = df['simplified_path'].apply(lambda l: l[1:-1])  # Adjust as per your input structure\n",
    "\n",
    "    # Initialize an empty DataFrame to store results\n",
    "    sum_cspeed_df = pd.DataFrame(columns=['article', 'n_appearances', 'sum_cspeed'])\n",
    "    sum_cspeed_df.set_index('article', inplace=True)\n",
    "\n",
    "    # Iterate through each row to calculate speeds\n",
    "    for _, row in df.iterrows():\n",
    "        cspeed = row['centered_speed']\n",
    "        simplified_path = row['simplified_path']\n",
    "\n",
    "        for article in simplified_path:\n",
    "            if article not in sum_cspeed_df.index:\n",
    "                sum_cspeed_df.loc[article] = [0, 0]\n",
    "\n",
    "            # Update counts and sums\n",
    "            sum_cspeed_df.at[article, 'n_appearances'] += 1\n",
    "            sum_cspeed_df.at[article, 'sum_cspeed'] += cspeed\n",
    "\n",
    "    # Filter out articles that appear less than the cutoff\n",
    "    sum_cspeed_df = sum_cspeed_df[sum_cspeed_df['n_appearances'] >= count_cutoff]\n",
    "\n",
    "    if scaling is not None:\n",
    "        # Normalize the sum of centered speeds\n",
    "        if scaling == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "        elif scaling == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        elif scaling == 'robust':\n",
    "            scaler = RobustScaler()\n",
    "        \n",
    "        sum_cspeed_df[scaling] = -scaler.fit_transform(sum_cspeed_df[['sum_cspeed']])\n",
    "    \n",
    "    print(f\"Number of unique articles after speed calc: {sum_cspeed_df.shape[0]}\")\n",
    "\n",
    "    return sum_cspeed_df  # Return the updated DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And a function for data filtering based on time aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_duration(df):\n",
    "    \"\"\"\n",
    "    Filter the DataFrame based on the distance and duration bounds using the IQR method. And downsample to one IpAdress per identifier.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame with the following columns:\n",
    "            - 'distance': Distance associated with the path\n",
    "            - 'durationInSec': Duration associated with the path\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame\n",
    "    \"\"\"\n",
    "    filtered_dfs = []  # List to hold filtered data for each distance group\n",
    "\n",
    "    for d in range(1, int(df['distance'].max()) + 1):\n",
    "        # Filter the DataFrame for the current distance group\n",
    "        df_d = df[df['distance'] == d]\n",
    "\n",
    "        # Compute IQR for 'durationInSec'\n",
    "        Q1 = df_d['durationInSec'].quantile(0.25)\n",
    "        Q3 = df_d['durationInSec'].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        # Calculate upper bound based on IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Keep only rows within the upper bound\n",
    "        filtered_df_d = df_d[df_d['durationInSec'] <= upper_bound]\n",
    "\n",
    "        # Append filtered group to the list\n",
    "        filtered_dfs.append(filtered_df_d)\n",
    "\n",
    "    # Concatenate all filtered groups\n",
    "    filtered_df = pd.concat(filtered_dfs, ignore_index=True)\n",
    "    \n",
    "    # downsample data to one IpAdress per identifier\n",
    "    downsampled_df = filtered_df.groupby(['hashedIpAddress', 'identifier']).sample(n=1, random_state=42)\n",
    "\n",
    "    # Calculate the number of removed rows\n",
    "    removed = df.shape[0] - downsampled_df.shape[0]\n",
    "\n",
    "    # Print the result\n",
    "    print(f\"In sampling a total of {removed} samples were removed, \"\n",
    "        f\"which represents {removed / df.shape[0] * 100:.3f}% of the original data.\",\n",
    "        f\"{df.shape[0]} samples remain.\")\n",
    "\n",
    "    return downsampled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a composite df with all the different scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_paths = feather.read_feather('Data/dataframes/filtered_paths.feather')\n",
    "\n",
    "finished_paths = filtered_paths[filtered_paths['finished']]\n",
    "\n",
    "# downsample data to one IpAdress per identifier\n",
    "# this way players can't just learn paths and then play them as fast as possible\n",
    "finished_paths = finished_paths.groupby(['hashedIpAddress', 'identifier']).sample(n=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sj/219f9qbn3y15yynshcyrrp3c0000gn/T/ipykernel_7008/1875624518.py:43: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.75' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  avg_article_weight_df.at[article, 'weighted_avg'] += weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique articles after weighting: 820\n",
      "In sampling a total of 2471 samples were removed, which represents 5.437% of the original data. 45451 samples remain.\n",
      "Number of unique articles after time calc: 776\n",
      "In sampling a total of 2471 samples were removed, which represents 5.437% of the original data. 45451 samples remain.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sj/219f9qbn3y15yynshcyrrp3c0000gn/T/ipykernel_7008/552281583.py:43: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '12.25' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  avg_article_speed_df.at[article, 'avg_speed'] += speed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique articles after speed calc: 776\n",
      "Ratio of unfinished over finished paths: 0.1762317738926128\n",
      "Number of unique articles after detour ratio calculation: 871\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "weight_df = calculate_avg_article_weights(finished_paths, count_cutoff=30, scaling='standard')\n",
    "time_df = calc_avg_article_time(filter_duration(finished_paths), count_cutoff=30, scaling='standard')\n",
    "speed_df = calc_avg_article_speed(filter_duration(finished_paths), count_cutoff=30, scaling='standard')\n",
    "unfinished_atio_df = calculate_unfinished_ratios(filtered_paths, count_cutoff=30, scaling='standard')\n",
    "detour_ratio_df = calculate_detour_ratios(finished_paths, count_cutoff=30, scaling='standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight_avg_scaled</th>\n",
       "      <th>weight_avg</th>\n",
       "      <th>unf_ratio_scaled</th>\n",
       "      <th>unfinished_ratio</th>\n",
       "      <th>detour_ratio_scaled</th>\n",
       "      <th>detour_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Philosophy</th>\n",
       "      <td>0.924846</td>\n",
       "      <td>0.643279</td>\n",
       "      <td>0.213956</td>\n",
       "      <td>0.186321</td>\n",
       "      <td>0.146164</td>\n",
       "      <td>0.056338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mathematics</th>\n",
       "      <td>0.893903</td>\n",
       "      <td>0.641757</td>\n",
       "      <td>0.896721</td>\n",
       "      <td>0.127168</td>\n",
       "      <td>1.029241</td>\n",
       "      <td>0.013699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arithmetic</th>\n",
       "      <td>1.100646</td>\n",
       "      <td>0.651925</td>\n",
       "      <td>0.953803</td>\n",
       "      <td>0.122222</td>\n",
       "      <td>-0.221156</td>\n",
       "      <td>0.074074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>North_Africa</th>\n",
       "      <td>-0.605665</td>\n",
       "      <td>0.568005</td>\n",
       "      <td>-0.090040</td>\n",
       "      <td>0.212658</td>\n",
       "      <td>0.155223</td>\n",
       "      <td>0.055901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Africa</th>\n",
       "      <td>0.907139</td>\n",
       "      <td>0.642408</td>\n",
       "      <td>0.529661</td>\n",
       "      <td>0.158969</td>\n",
       "      <td>0.973283</td>\n",
       "      <td>0.016401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United_States_Senate</th>\n",
       "      <td>-1.257078</td>\n",
       "      <td>0.535967</td>\n",
       "      <td>-1.073610</td>\n",
       "      <td>0.297872</td>\n",
       "      <td>-2.311367</td>\n",
       "      <td>0.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cheese</th>\n",
       "      <td>2.737321</td>\n",
       "      <td>0.732421</td>\n",
       "      <td>0.492802</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>0.644868</td>\n",
       "      <td>0.032258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nobel_Peace_Prize</th>\n",
       "      <td>0.919959</td>\n",
       "      <td>0.643039</td>\n",
       "      <td>0.569058</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>0.193466</td>\n",
       "      <td>0.054054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Triassic</th>\n",
       "      <td>-0.615929</td>\n",
       "      <td>0.567500</td>\n",
       "      <td>-0.299084</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>-1.645677</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marxism</th>\n",
       "      <td>1.915060</td>\n",
       "      <td>0.691980</td>\n",
       "      <td>0.154298</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.795185</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>820 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      weight_avg_scaled  weight_avg  unf_ratio_scaled  \\\n",
       "article                                                                 \n",
       "Philosophy                     0.924846    0.643279          0.213956   \n",
       "Mathematics                    0.893903    0.641757          0.896721   \n",
       "Arithmetic                     1.100646    0.651925          0.953803   \n",
       "North_Africa                  -0.605665    0.568005         -0.090040   \n",
       "Africa                         0.907139    0.642408          0.529661   \n",
       "...                                 ...         ...               ...   \n",
       "United_States_Senate          -1.257078    0.535967         -1.073610   \n",
       "Cheese                         2.737321    0.732421          0.492802   \n",
       "Nobel_Peace_Prize              0.919959    0.643039          0.569058   \n",
       "Triassic                      -0.615929    0.567500         -0.299084   \n",
       "Marxism                        1.915060    0.691980          0.154298   \n",
       "\n",
       "                      unfinished_ratio  detour_ratio_scaled  detour_ratio  \n",
       "article                                                                    \n",
       "Philosophy                    0.186321             0.146164      0.056338  \n",
       "Mathematics                   0.127168             1.029241      0.013699  \n",
       "Arithmetic                    0.122222            -0.221156      0.074074  \n",
       "North_Africa                  0.212658             0.155223      0.055901  \n",
       "Africa                        0.158969             0.973283      0.016401  \n",
       "...                                ...                  ...           ...  \n",
       "United_States_Senate          0.297872            -2.311367      0.175000  \n",
       "Cheese                        0.162162             0.644868      0.032258  \n",
       "Nobel_Peace_Prize             0.155556             0.193466      0.054054  \n",
       "Triassic                      0.230769            -1.645677      0.142857  \n",
       "Marxism                       0.191489             0.795185      0.025000  \n",
       "\n",
       "[820 rows x 6 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the metrics into a composite score\n",
    "composite_df = pd.DataFrame(index=weight_df.index)\n",
    "composite_df['weight_avg_scaled'] = weight_df['standard']\n",
    "composite_df['weight_avg'] = weight_df['weighted_avg']\n",
    "\n",
    "composite_df['unf_ratio_scaled'] = unfinished_atio_df['standard']\n",
    "composite_df['unfinished_ratio'] = unfinished_atio_df['unfinished_ratio']\n",
    "\n",
    "composite_df['detour_ratio_scaled'] = detour_ratio_df['standard']\n",
    "composite_df['detour_ratio'] = detour_ratio_df['detour_ratio']\n",
    "\n",
    "composite_df1 = pd.DataFrame(index=time_df.index)\n",
    "composite_df1['avg_time_scaled'] = time_df['standard']\n",
    "composite_df1['avg_time'] = time_df['avg_speed']\n",
    "\n",
    "composite_df1['speed_scaled'] = speed_df['standard']\n",
    "composite_df1['speed'] = speed_df['avg_speed']\n",
    "\n",
    "composite_df\n",
    "\n",
    "# | article | weighted_avg | avg_speed | unfinished_ratio | detour_ratio |\n",
    "# |---------|bigger better |small better|   small better  | small better |\n",
    "# the sclaed version is always the bigger the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight_avg_scaled</th>\n",
       "      <th>weight_avg</th>\n",
       "      <th>unf_ratio_scaled</th>\n",
       "      <th>unfinished_ratio</th>\n",
       "      <th>detour_ratio_scaled</th>\n",
       "      <th>detour_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Achilles</th>\n",
       "      <td>4.811790</td>\n",
       "      <td>0.834448</td>\n",
       "      <td>0.850784</td>\n",
       "      <td>0.131148</td>\n",
       "      <td>1.312944</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>J._K._Rowling</th>\n",
       "      <td>4.044191</td>\n",
       "      <td>0.796696</td>\n",
       "      <td>1.006611</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.949604</td>\n",
       "      <td>0.017544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mario</th>\n",
       "      <td>3.390951</td>\n",
       "      <td>0.764568</td>\n",
       "      <td>0.859010</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.277426</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Harry_Potter</th>\n",
       "      <td>3.188972</td>\n",
       "      <td>0.754634</td>\n",
       "      <td>0.565727</td>\n",
       "      <td>0.155844</td>\n",
       "      <td>1.312944</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lead</th>\n",
       "      <td>3.156074</td>\n",
       "      <td>0.753016</td>\n",
       "      <td>0.492802</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>-3.047129</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anatomy</th>\n",
       "      <td>-2.276821</td>\n",
       "      <td>0.485813</td>\n",
       "      <td>-2.486595</td>\n",
       "      <td>0.420290</td>\n",
       "      <td>-0.569815</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Irrigation</th>\n",
       "      <td>-2.391556</td>\n",
       "      <td>0.480171</td>\n",
       "      <td>-1.989156</td>\n",
       "      <td>0.377193</td>\n",
       "      <td>-0.646143</td>\n",
       "      <td>0.094595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gas</th>\n",
       "      <td>-2.539718</td>\n",
       "      <td>0.472884</td>\n",
       "      <td>-0.473747</td>\n",
       "      <td>0.245902</td>\n",
       "      <td>-3.601376</td>\n",
       "      <td>0.237288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Atheism</th>\n",
       "      <td>-2.545097</td>\n",
       "      <td>0.472619</td>\n",
       "      <td>0.265926</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>-0.412918</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actor</th>\n",
       "      <td>-2.722738</td>\n",
       "      <td>0.463882</td>\n",
       "      <td>-3.824837</td>\n",
       "      <td>0.536232</td>\n",
       "      <td>-1.563493</td>\n",
       "      <td>0.138889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>820 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               weight_avg_scaled  weight_avg  unf_ratio_scaled  \\\n",
       "article                                                          \n",
       "Achilles                4.811790    0.834448          0.850784   \n",
       "J._K._Rowling           4.044191    0.796696          1.006611   \n",
       "Mario                   3.390951    0.764568          0.859010   \n",
       "Harry_Potter            3.188972    0.754634          0.565727   \n",
       "Lead                    3.156074    0.753016          0.492802   \n",
       "...                          ...         ...               ...   \n",
       "Anatomy                -2.276821    0.485813         -2.486595   \n",
       "Irrigation             -2.391556    0.480171         -1.989156   \n",
       "Gas                    -2.539718    0.472884         -0.473747   \n",
       "Atheism                -2.545097    0.472619          0.265926   \n",
       "Actor                  -2.722738    0.463882         -3.824837   \n",
       "\n",
       "               unfinished_ratio  detour_ratio_scaled  detour_ratio  \n",
       "article                                                             \n",
       "Achilles               0.131148             1.312944      0.000000  \n",
       "J._K._Rowling          0.117647             0.949604      0.017544  \n",
       "Mario                  0.130435             0.277426      0.050000  \n",
       "Harry_Potter           0.155844             1.312944      0.000000  \n",
       "Lead                   0.162162            -3.047129      0.210526  \n",
       "...                         ...                  ...           ...  \n",
       "Anatomy                0.420290            -0.569815      0.090909  \n",
       "Irrigation             0.377193            -0.646143      0.094595  \n",
       "Gas                    0.245902            -3.601376      0.237288  \n",
       "Atheism                0.181818            -0.412918      0.083333  \n",
       "Actor                  0.536232            -1.563493      0.138889  \n",
       "\n",
       "[820 rows x 6 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rank by highest wieght (remember weight for an article is the average of (distance / simplified_path_length) over all the paths it appears in)\n",
    "composite_df.sort_values(by='weight_avg', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight_avg_scaled</th>\n",
       "      <th>weight_avg</th>\n",
       "      <th>unf_ratio_scaled</th>\n",
       "      <th>unfinished_ratio</th>\n",
       "      <th>detour_ratio_scaled</th>\n",
       "      <th>detour_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Frog</th>\n",
       "      <td>1.556368</td>\n",
       "      <td>0.674339</td>\n",
       "      <td>2.364533</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.644868</td>\n",
       "      <td>0.032258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Australian_Green_Tree_Frog</th>\n",
       "      <td>1.008014</td>\n",
       "      <td>0.647369</td>\n",
       "      <td>2.364533</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.795185</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>List_of_countries</th>\n",
       "      <td>0.528985</td>\n",
       "      <td>0.623810</td>\n",
       "      <td>2.096106</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>1.312944</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Periodic_table</th>\n",
       "      <td>1.216158</td>\n",
       "      <td>0.657606</td>\n",
       "      <td>2.081005</td>\n",
       "      <td>0.024564</td>\n",
       "      <td>1.089883</td>\n",
       "      <td>0.010771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kuwait</th>\n",
       "      <td>-0.397505</td>\n",
       "      <td>0.578243</td>\n",
       "      <td>2.043912</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.193466</td>\n",
       "      <td>0.054054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fiction</th>\n",
       "      <td>-0.996283</td>\n",
       "      <td>0.548793</td>\n",
       "      <td>-3.030255</td>\n",
       "      <td>0.467391</td>\n",
       "      <td>-1.593772</td>\n",
       "      <td>0.140351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The_Simpsons</th>\n",
       "      <td>0.802556</td>\n",
       "      <td>0.637264</td>\n",
       "      <td>-3.148226</td>\n",
       "      <td>0.477612</td>\n",
       "      <td>-0.988206</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sport</th>\n",
       "      <td>-0.963159</td>\n",
       "      <td>0.550422</td>\n",
       "      <td>-3.551640</td>\n",
       "      <td>0.512563</td>\n",
       "      <td>-2.288856</td>\n",
       "      <td>0.173913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mexico_City</th>\n",
       "      <td>-0.744954</td>\n",
       "      <td>0.561154</td>\n",
       "      <td>-3.581520</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>-0.311397</td>\n",
       "      <td>0.078431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actor</th>\n",
       "      <td>-2.722738</td>\n",
       "      <td>0.463882</td>\n",
       "      <td>-3.824837</td>\n",
       "      <td>0.536232</td>\n",
       "      <td>-1.563493</td>\n",
       "      <td>0.138889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>820 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            weight_avg_scaled  weight_avg  unf_ratio_scaled  \\\n",
       "article                                                                       \n",
       "Frog                                 1.556368    0.674339          2.364533   \n",
       "Australian_Green_Tree_Frog           1.008014    0.647369          2.364533   \n",
       "List_of_countries                    0.528985    0.623810          2.096106   \n",
       "Periodic_table                       1.216158    0.657606          2.081005   \n",
       "Kuwait                              -0.397505    0.578243          2.043912   \n",
       "...                                       ...         ...               ...   \n",
       "Fiction                             -0.996283    0.548793         -3.030255   \n",
       "The_Simpsons                         0.802556    0.637264         -3.148226   \n",
       "Sport                               -0.963159    0.550422         -3.551640   \n",
       "Mexico_City                         -0.744954    0.561154         -3.581520   \n",
       "Actor                               -2.722738    0.463882         -3.824837   \n",
       "\n",
       "                            unfinished_ratio  detour_ratio_scaled  \\\n",
       "article                                                             \n",
       "Frog                                0.000000             0.644868   \n",
       "Australian_Green_Tree_Frog          0.000000             0.795185   \n",
       "List_of_countries                   0.023256             1.312944   \n",
       "Periodic_table                      0.024564             1.089883   \n",
       "Kuwait                              0.027778             0.193466   \n",
       "...                                      ...                  ...   \n",
       "Fiction                             0.467391            -1.593772   \n",
       "The_Simpsons                        0.477612            -0.988206   \n",
       "Sport                               0.512563            -2.288856   \n",
       "Mexico_City                         0.515152            -0.311397   \n",
       "Actor                               0.536232            -1.563493   \n",
       "\n",
       "                            detour_ratio  \n",
       "article                                   \n",
       "Frog                            0.032258  \n",
       "Australian_Green_Tree_Frog      0.025000  \n",
       "List_of_countries               0.000000  \n",
       "Periodic_table                  0.010771  \n",
       "Kuwait                          0.054054  \n",
       "...                                  ...  \n",
       "Fiction                         0.140351  \n",
       "The_Simpsons                    0.111111  \n",
       "Sport                           0.173913  \n",
       "Mexico_City                     0.078431  \n",
       "Actor                           0.138889  \n",
       "\n",
       "[820 rows x 6 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort by unfinished ratio (ratio of the number of times an article appears in unfinished paths over the total number of times it appears)\n",
    "composite_df.sort_values(by='unf_ratio_scaled', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight_avg_scaled</th>\n",
       "      <th>weight_avg</th>\n",
       "      <th>unf_ratio_scaled</th>\n",
       "      <th>unfinished_ratio</th>\n",
       "      <th>detour_ratio_scaled</th>\n",
       "      <th>detour_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Detroit,_Michigan</th>\n",
       "      <td>0.848160</td>\n",
       "      <td>0.639507</td>\n",
       "      <td>0.748606</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>1.312944</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Star_Wars</th>\n",
       "      <td>0.208158</td>\n",
       "      <td>0.608031</td>\n",
       "      <td>-0.185519</td>\n",
       "      <td>0.220930</td>\n",
       "      <td>1.312944</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Star_Wars_Episode_IV__A_New_Hope</th>\n",
       "      <td>1.880626</td>\n",
       "      <td>0.690286</td>\n",
       "      <td>0.973890</td>\n",
       "      <td>0.120482</td>\n",
       "      <td>1.312944</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>History_of_the_world</th>\n",
       "      <td>-0.448221</td>\n",
       "      <td>0.575748</td>\n",
       "      <td>-1.186956</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>1.312944</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bread</th>\n",
       "      <td>-0.056651</td>\n",
       "      <td>0.595007</td>\n",
       "      <td>1.498857</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>1.312944</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yellowstone_National_Park</th>\n",
       "      <td>-0.989639</td>\n",
       "      <td>0.549120</td>\n",
       "      <td>-2.118855</td>\n",
       "      <td>0.388430</td>\n",
       "      <td>-3.289356</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eukaryote</th>\n",
       "      <td>-1.550714</td>\n",
       "      <td>0.521525</td>\n",
       "      <td>-2.631404</td>\n",
       "      <td>0.432836</td>\n",
       "      <td>-3.433178</td>\n",
       "      <td>0.229167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gas</th>\n",
       "      <td>-2.539718</td>\n",
       "      <td>0.472884</td>\n",
       "      <td>-0.473747</td>\n",
       "      <td>0.245902</td>\n",
       "      <td>-3.601376</td>\n",
       "      <td>0.237288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVD</th>\n",
       "      <td>-1.089802</td>\n",
       "      <td>0.544194</td>\n",
       "      <td>-2.182449</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>-3.966165</td>\n",
       "      <td>0.254902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Optical_fiber</th>\n",
       "      <td>-1.738252</td>\n",
       "      <td>0.512302</td>\n",
       "      <td>-0.867322</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>-5.713781</td>\n",
       "      <td>0.339286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>820 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  weight_avg_scaled  weight_avg  \\\n",
       "article                                                           \n",
       "Detroit,_Michigan                          0.848160    0.639507   \n",
       "Star_Wars                                  0.208158    0.608031   \n",
       "Star_Wars_Episode_IV__A_New_Hope           1.880626    0.690286   \n",
       "History_of_the_world                      -0.448221    0.575748   \n",
       "Bread                                     -0.056651    0.595007   \n",
       "...                                             ...         ...   \n",
       "Yellowstone_National_Park                 -0.989639    0.549120   \n",
       "Eukaryote                                 -1.550714    0.521525   \n",
       "Gas                                       -2.539718    0.472884   \n",
       "DVD                                       -1.089802    0.544194   \n",
       "Optical_fiber                             -1.738252    0.512302   \n",
       "\n",
       "                                  unf_ratio_scaled  unfinished_ratio  \\\n",
       "article                                                                \n",
       "Detroit,_Michigan                         0.748606          0.140000   \n",
       "Star_Wars                                -0.185519          0.220930   \n",
       "Star_Wars_Episode_IV__A_New_Hope          0.973890          0.120482   \n",
       "History_of_the_world                     -1.186956          0.307692   \n",
       "Bread                                     1.498857          0.075000   \n",
       "...                                            ...               ...   \n",
       "Yellowstone_National_Park                -2.118855          0.388430   \n",
       "Eukaryote                                -2.631404          0.432836   \n",
       "Gas                                      -0.473747          0.245902   \n",
       "DVD                                      -2.182449          0.393939   \n",
       "Optical_fiber                            -0.867322          0.280000   \n",
       "\n",
       "                                  detour_ratio_scaled  detour_ratio  \n",
       "article                                                              \n",
       "Detroit,_Michigan                            1.312944      0.000000  \n",
       "Star_Wars                                    1.312944      0.000000  \n",
       "Star_Wars_Episode_IV__A_New_Hope             1.312944      0.000000  \n",
       "History_of_the_world                         1.312944      0.000000  \n",
       "Bread                                        1.312944      0.000000  \n",
       "...                                               ...           ...  \n",
       "Yellowstone_National_Park                   -3.289356      0.222222  \n",
       "Eukaryote                                   -3.433178      0.229167  \n",
       "Gas                                         -3.601376      0.237288  \n",
       "DVD                                         -3.966165      0.254902  \n",
       "Optical_fiber                               -5.713781      0.339286  \n",
       "\n",
       "[820 rows x 6 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort by detour ratio (ratio of the number of dead ends an article has (difference between full path list content and simplified path list content))\n",
    "composite_df.sort_values(by='detour_ratio_scaled', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_time_scaled</th>\n",
       "      <th>avg_time</th>\n",
       "      <th>speed_scaled</th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>North_Korea</th>\n",
       "      <td>3.003798</td>\n",
       "      <td>84.780488</td>\n",
       "      <td>1.775854</td>\n",
       "      <td>19.936934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Old_English_language</th>\n",
       "      <td>2.821436</td>\n",
       "      <td>87.923077</td>\n",
       "      <td>2.919625</td>\n",
       "      <td>17.122426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korea</th>\n",
       "      <td>2.687098</td>\n",
       "      <td>90.238095</td>\n",
       "      <td>1.905683</td>\n",
       "      <td>19.617460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Suez_Canal</th>\n",
       "      <td>2.453527</td>\n",
       "      <td>94.263158</td>\n",
       "      <td>2.235149</td>\n",
       "      <td>18.806736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>President_of_the_United_States</th>\n",
       "      <td>2.117642</td>\n",
       "      <td>100.051360</td>\n",
       "      <td>1.245681</td>\n",
       "      <td>21.241544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Malaria</th>\n",
       "      <td>-3.014132</td>\n",
       "      <td>188.485714</td>\n",
       "      <td>-2.228086</td>\n",
       "      <td>29.789535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005_Atlantic_hurricane_season</th>\n",
       "      <td>-3.190475</td>\n",
       "      <td>191.524590</td>\n",
       "      <td>-1.916227</td>\n",
       "      <td>29.022135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Detroit,_Michigan</th>\n",
       "      <td>-3.627169</td>\n",
       "      <td>199.050000</td>\n",
       "      <td>-3.159923</td>\n",
       "      <td>32.082531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extrasolar_planet</th>\n",
       "      <td>-3.678670</td>\n",
       "      <td>199.937500</td>\n",
       "      <td>-2.367974</td>\n",
       "      <td>30.133761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Saturn_V</th>\n",
       "      <td>-5.348563</td>\n",
       "      <td>228.714286</td>\n",
       "      <td>-4.229965</td>\n",
       "      <td>34.715612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>776 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                avg_time_scaled    avg_time  speed_scaled  \\\n",
       "article                                                                     \n",
       "North_Korea                            3.003798   84.780488      1.775854   \n",
       "Old_English_language                   2.821436   87.923077      2.919625   \n",
       "Korea                                  2.687098   90.238095      1.905683   \n",
       "Suez_Canal                             2.453527   94.263158      2.235149   \n",
       "President_of_the_United_States         2.117642  100.051360      1.245681   \n",
       "...                                         ...         ...           ...   \n",
       "Malaria                               -3.014132  188.485714     -2.228086   \n",
       "2005_Atlantic_hurricane_season        -3.190475  191.524590     -1.916227   \n",
       "Detroit,_Michigan                     -3.627169  199.050000     -3.159923   \n",
       "Extrasolar_planet                     -3.678670  199.937500     -2.367974   \n",
       "Saturn_V                              -5.348563  228.714286     -4.229965   \n",
       "\n",
       "                                    speed  \n",
       "article                                    \n",
       "North_Korea                     19.936934  \n",
       "Old_English_language            17.122426  \n",
       "Korea                           19.617460  \n",
       "Suez_Canal                      18.806736  \n",
       "President_of_the_United_States  21.241544  \n",
       "...                                   ...  \n",
       "Malaria                         29.789535  \n",
       "2005_Atlantic_hurricane_season  29.022135  \n",
       "Detroit,_Michigan               32.082531  \n",
       "Extrasolar_planet               30.133761  \n",
       "Saturn_V                        34.715612  \n",
       "\n",
       "[776 rows x 4 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort by speed (so far speed is just avg path time over all the paths an article appears in)\n",
    "composite_df1.sort_values(by='avg_time_scaled', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What now?\n",
    "\n",
    "It would be cool to **get a composite score that incorporates all the metrics but what weight do we give each individual metric...?**\n",
    "- PCA\n",
    "- Weighted average\n",
    "\n",
    "We can also seperate the game into two main objectives:\n",
    "- **Reach your target in the least possible amount of clicks**.\n",
    "    consider the following metrics\n",
    "    - weighted_avg score\n",
    "    - detour ratio \n",
    "    - maybe unfinished ratio\n",
    "- **Reach your target as fast as possible**\n",
    "    only really interested in time metric\n",
    "\n",
    "Then we can test what article attributes correlate the most with high scores. And if they are similar for clicks and time.\n",
    "\n",
    "Note that these metrics are more focused on article 'quality'. what I mean by that is that it is not the most important articles in the 'Network' (i.e. those that are the most used by players) that will have the highest scores.\n",
    "\n",
    "\n",
    "**TO DO**\n",
    "\n",
    "- code the centered sum of weights function\n",
    "\n",
    "- explain exactly each metric\n",
    "\n",
    "- compute composite scores \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First PCA on the three scores (click related)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight_avg_scaled</th>\n",
       "      <th>unf_ratio_scaled</th>\n",
       "      <th>detour_ratio_scaled</th>\n",
       "      <th>composite_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Achilles</th>\n",
       "      <td>4.811790</td>\n",
       "      <td>0.850784</td>\n",
       "      <td>1.312944</td>\n",
       "      <td>4.158275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>J._K._Rowling</th>\n",
       "      <td>4.044191</td>\n",
       "      <td>1.006611</td>\n",
       "      <td>0.949604</td>\n",
       "      <td>3.559448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algebra</th>\n",
       "      <td>2.421177</td>\n",
       "      <td>1.664997</td>\n",
       "      <td>1.312944</td>\n",
       "      <td>3.068307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Harry_Potter</th>\n",
       "      <td>3.188972</td>\n",
       "      <td>0.565727</td>\n",
       "      <td>1.312944</td>\n",
       "      <td>2.952162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parrot</th>\n",
       "      <td>2.304506</td>\n",
       "      <td>1.180703</td>\n",
       "      <td>1.312944</td>\n",
       "      <td>2.723494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sport</th>\n",
       "      <td>-0.963159</td>\n",
       "      <td>-3.551640</td>\n",
       "      <td>-2.288856</td>\n",
       "      <td>-3.903475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVD</th>\n",
       "      <td>-1.089802</td>\n",
       "      <td>-2.182449</td>\n",
       "      <td>-3.966165</td>\n",
       "      <td>-4.100757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eukaryote</th>\n",
       "      <td>-1.550714</td>\n",
       "      <td>-2.631404</td>\n",
       "      <td>-3.433178</td>\n",
       "      <td>-4.369271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Optical_fiber</th>\n",
       "      <td>-1.738252</td>\n",
       "      <td>-0.867322</td>\n",
       "      <td>-5.713781</td>\n",
       "      <td>-4.701717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actor</th>\n",
       "      <td>-2.722738</td>\n",
       "      <td>-3.824837</td>\n",
       "      <td>-1.563493</td>\n",
       "      <td>-4.811791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>820 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               weight_avg_scaled  unf_ratio_scaled  detour_ratio_scaled  \\\n",
       "article                                                                   \n",
       "Achilles                4.811790          0.850784             1.312944   \n",
       "J._K._Rowling           4.044191          1.006611             0.949604   \n",
       "Algebra                 2.421177          1.664997             1.312944   \n",
       "Harry_Potter            3.188972          0.565727             1.312944   \n",
       "Parrot                  2.304506          1.180703             1.312944   \n",
       "...                          ...               ...                  ...   \n",
       "Sport                  -0.963159         -3.551640            -2.288856   \n",
       "DVD                    -1.089802         -2.182449            -3.966165   \n",
       "Eukaryote              -1.550714         -2.631404            -3.433178   \n",
       "Optical_fiber          -1.738252         -0.867322            -5.713781   \n",
       "Actor                  -2.722738         -3.824837            -1.563493   \n",
       "\n",
       "               composite_score  \n",
       "article                         \n",
       "Achilles              4.158275  \n",
       "J._K._Rowling         3.559448  \n",
       "Algebra               3.068307  \n",
       "Harry_Potter          2.952162  \n",
       "Parrot                2.723494  \n",
       "...                        ...  \n",
       "Sport                -3.903475  \n",
       "DVD                  -4.100757  \n",
       "Eukaryote            -4.369271  \n",
       "Optical_fiber        -4.701717  \n",
       "Actor                -4.811791  \n",
       "\n",
       "[820 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Ensure all metrics are such that \"bigger is better\"\n",
    "composite1_df = composite_df[['weight_avg_scaled', 'unf_ratio_scaled', 'detour_ratio_scaled']].copy()\n",
    "\n",
    "# Apply PCA to get a single composite score\n",
    "pca = PCA(n_components=1)\n",
    "composite1_df['composite_score'] = pca.fit_transform(composite1_df)\n",
    "composite1_df.sort_values(by='composite_score', ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA only on weight and detour scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight_avg_scaled</th>\n",
       "      <th>detour_ratio_scaled</th>\n",
       "      <th>composite_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Achilles</th>\n",
       "      <td>4.811790</td>\n",
       "      <td>1.312944</td>\n",
       "      <td>4.630274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>J._K._Rowling</th>\n",
       "      <td>4.044191</td>\n",
       "      <td>0.949604</td>\n",
       "      <td>3.794620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Harry_Potter</th>\n",
       "      <td>3.188972</td>\n",
       "      <td>1.312944</td>\n",
       "      <td>3.310698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mario</th>\n",
       "      <td>3.390951</td>\n",
       "      <td>0.277426</td>\n",
       "      <td>2.872191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algebra</th>\n",
       "      <td>2.421177</td>\n",
       "      <td>1.312944</td>\n",
       "      <td>2.686373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Herbivore</th>\n",
       "      <td>-1.964565</td>\n",
       "      <td>-2.680531</td>\n",
       "      <td>-3.204325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVD</th>\n",
       "      <td>-1.089802</td>\n",
       "      <td>-3.966165</td>\n",
       "      <td>-3.241349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eukaryote</th>\n",
       "      <td>-1.550714</td>\n",
       "      <td>-3.433178</td>\n",
       "      <td>-3.305899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gas</th>\n",
       "      <td>-2.539718</td>\n",
       "      <td>-3.601376</td>\n",
       "      <td>-4.208000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Optical_fiber</th>\n",
       "      <td>-1.738252</td>\n",
       "      <td>-5.713781</td>\n",
       "      <td>-4.785863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>820 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               weight_avg_scaled  detour_ratio_scaled  composite_score\n",
       "article                                                               \n",
       "Achilles                4.811790             1.312944         4.630274\n",
       "J._K._Rowling           4.044191             0.949604         3.794620\n",
       "Harry_Potter            3.188972             1.312944         3.310698\n",
       "Mario                   3.390951             0.277426         2.872191\n",
       "Algebra                 2.421177             1.312944         2.686373\n",
       "...                          ...                  ...              ...\n",
       "Herbivore              -1.964565            -2.680531        -3.204325\n",
       "DVD                    -1.089802            -3.966165        -3.241349\n",
       "Eukaryote              -1.550714            -3.433178        -3.305899\n",
       "Gas                    -2.539718            -3.601376        -4.208000\n",
       "Optical_fiber          -1.738252            -5.713781        -4.785863\n",
       "\n",
       "[820 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure all metrics are such that \"bigger is better\"\n",
    "composite2_df = composite_df[['weight_avg_scaled', 'detour_ratio_scaled']].copy()\n",
    "\n",
    "# Apply PCA to get a single composite score\n",
    "pca = PCA(n_components=1)\n",
    "composite2_df['composite_score'] = pca.fit_transform(composite2_df)\n",
    "composite2_df.sort_values(by='composite_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sj/219f9qbn3y15yynshcyrrp3c0000gn/T/ipykernel_10877/4190968862.py:119: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.13976087385302072' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  sum_article_cweight_df.at[article, 'weighted_sum'] += cweight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique articles after weighting: 820\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_appearances</th>\n",
       "      <th>weighted_sum</th>\n",
       "      <th>standard</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>United_States</th>\n",
       "      <td>7294</td>\n",
       "      <td>499.457408</td>\n",
       "      <td>23.946411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United_Kingdom</th>\n",
       "      <td>3180</td>\n",
       "      <td>134.954442</td>\n",
       "      <td>6.448414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>World_War_II</th>\n",
       "      <td>1857</td>\n",
       "      <td>92.022451</td>\n",
       "      <td>4.387460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Europe</th>\n",
       "      <td>3643</td>\n",
       "      <td>83.214123</td>\n",
       "      <td>3.964615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Africa</th>\n",
       "      <td>2271</td>\n",
       "      <td>73.055639</td>\n",
       "      <td>3.476956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Continent</th>\n",
       "      <td>496</td>\n",
       "      <td>-28.802416</td>\n",
       "      <td>-1.412749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <td>294</td>\n",
       "      <td>-29.516652</td>\n",
       "      <td>-1.447036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Technology</th>\n",
       "      <td>491</td>\n",
       "      <td>-32.325643</td>\n",
       "      <td>-1.581882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Plant</th>\n",
       "      <td>788</td>\n",
       "      <td>-32.887695</td>\n",
       "      <td>-1.608863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Science</th>\n",
       "      <td>1101</td>\n",
       "      <td>-54.829050</td>\n",
       "      <td>-2.662160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>820 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                n_appearances  weighted_sum   standard\n",
       "article                                               \n",
       "United_States            7294    499.457408  23.946411\n",
       "United_Kingdom           3180    134.954442   6.448414\n",
       "World_War_II             1857     92.022451   4.387460\n",
       "Europe                   3643     83.214123   3.964615\n",
       "Africa                   2271     73.055639   3.476956\n",
       "...                       ...           ...        ...\n",
       "Continent                 496    -28.802416  -1.412749\n",
       "Country                   294    -29.516652  -1.447036\n",
       "Technology                491    -32.325643  -1.581882\n",
       "Plant                     788    -32.887695  -1.608863\n",
       "Science                  1101    -54.829050  -2.662160\n",
       "\n",
       "[820 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1 = calculate_sum_article_cweights(finished_paths, count_cutoff=30, scaling='standard').sort_values(by='standard', ascending=False)\n",
    "c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In sampling a total of 17779 samples were removed, which represents 39.117% of the original data. 27672 samples remain.\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "shuffled_df = finished_paths.sample(frac=1).reset_index(drop=True)\n",
    "filtered_paths_sampled = shuffled_df.drop_duplicates(subset='identifier', keep='first').reset_index(drop=True)\n",
    "\n",
    "# Calculate the number of removed rows\n",
    "removed = finished_paths.shape[0] - filtered_paths_sampled.shape[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"In sampling a total of {removed} samples were removed, \"\n",
    "      f\"which represents {removed / finished_paths.shape[0] * 100:.3f}% of the original data.\",\n",
    "      f\"{filtered_paths_sampled.shape[0]} samples remain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sj/219f9qbn3y15yynshcyrrp3c0000gn/T/ipykernel_10877/4190968862.py:119: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.11836807862219512' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  sum_article_cweight_df.at[article, 'weighted_sum'] += cweight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique articles after weighting: 564\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_appearances</th>\n",
       "      <th>weighted_sum</th>\n",
       "      <th>standard</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>United_States</th>\n",
       "      <td>4527</td>\n",
       "      <td>339.638331</td>\n",
       "      <td>20.486670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United_Kingdom</th>\n",
       "      <td>2105</td>\n",
       "      <td>70.501313</td>\n",
       "      <td>4.209643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Africa</th>\n",
       "      <td>1350</td>\n",
       "      <td>65.920499</td>\n",
       "      <td>3.932602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>World_War_II</th>\n",
       "      <td>1111</td>\n",
       "      <td>60.037888</td>\n",
       "      <td>3.576830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Europe</th>\n",
       "      <td>2171</td>\n",
       "      <td>59.978866</td>\n",
       "      <td>3.573260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Great_Britain</th>\n",
       "      <td>290</td>\n",
       "      <td>-16.585956</td>\n",
       "      <td>-1.057272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <td>168</td>\n",
       "      <td>-18.364956</td>\n",
       "      <td>-1.164864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Continent</th>\n",
       "      <td>307</td>\n",
       "      <td>-20.696714</td>\n",
       "      <td>-1.305885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chemical_element</th>\n",
       "      <td>391</td>\n",
       "      <td>-22.440303</td>\n",
       "      <td>-1.411335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Science</th>\n",
       "      <td>623</td>\n",
       "      <td>-37.130713</td>\n",
       "      <td>-2.299790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>564 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  n_appearances  weighted_sum   standard\n",
       "article                                                 \n",
       "United_States              4527    339.638331  20.486670\n",
       "United_Kingdom             2105     70.501313   4.209643\n",
       "Africa                     1350     65.920499   3.932602\n",
       "World_War_II               1111     60.037888   3.576830\n",
       "Europe                     2171     59.978866   3.573260\n",
       "...                         ...           ...        ...\n",
       "Great_Britain               290    -16.585956  -1.057272\n",
       "Country                     168    -18.364956  -1.164864\n",
       "Continent                   307    -20.696714  -1.305885\n",
       "Chemical_element            391    -22.440303  -1.411335\n",
       "Science                     623    -37.130713  -2.299790\n",
       "\n",
       "[564 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2 = calculate_sum_article_cweights(filtered_paths_sampled, count_cutoff=30, scaling='standard').sort_values(by='standard', ascending=False)\n",
    "c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In sampling a total of 2471 samples were removed, which represents 5.437% of the original data. 45451 samples remain.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sj/219f9qbn3y15yynshcyrrp3c0000gn/T/ipykernel_10877/1974316627.py:117: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '-11.941829564904062' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  sum_cspeed_df.at[article, 'sum_cspeed'] += cspeed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique articles after speed calc: 776\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_appearances</th>\n",
       "      <th>sum_cspeed</th>\n",
       "      <th>standard</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Europe</th>\n",
       "      <td>3468</td>\n",
       "      <td>-6349.844513</td>\n",
       "      <td>11.767556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>North_America</th>\n",
       "      <td>1334</td>\n",
       "      <td>-4515.373655</td>\n",
       "      <td>8.351842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Earth</th>\n",
       "      <td>2403</td>\n",
       "      <td>-4027.153638</td>\n",
       "      <td>7.442795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asia</th>\n",
       "      <td>924</td>\n",
       "      <td>-3039.193375</td>\n",
       "      <td>5.603251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Continent</th>\n",
       "      <td>480</td>\n",
       "      <td>-2910.773032</td>\n",
       "      <td>5.364137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Computer</th>\n",
       "      <td>968</td>\n",
       "      <td>2071.441268</td>\n",
       "      <td>-3.912554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agriculture</th>\n",
       "      <td>730</td>\n",
       "      <td>2116.984656</td>\n",
       "      <td>-3.997354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United_States</th>\n",
       "      <td>6880</td>\n",
       "      <td>2323.351410</td>\n",
       "      <td>-4.381601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United_Kingdom</th>\n",
       "      <td>2962</td>\n",
       "      <td>2415.395634</td>\n",
       "      <td>-4.552984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>World_War_II</th>\n",
       "      <td>1712</td>\n",
       "      <td>2677.549329</td>\n",
       "      <td>-5.041104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>776 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                n_appearances   sum_cspeed   standard\n",
       "article                                              \n",
       "Europe                   3468 -6349.844513  11.767556\n",
       "North_America            1334 -4515.373655   8.351842\n",
       "Earth                    2403 -4027.153638   7.442795\n",
       "Asia                      924 -3039.193375   5.603251\n",
       "Continent                 480 -2910.773032   5.364137\n",
       "...                       ...          ...        ...\n",
       "Computer                  968  2071.441268  -3.912554\n",
       "Agriculture               730  2116.984656  -3.997354\n",
       "United_States            6880  2323.351410  -4.381601\n",
       "United_Kingdom           2962  2415.395634  -4.552984\n",
       "World_War_II             1712  2677.549329  -5.041104\n",
       "\n",
       "[776 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs1 = calc_sum_article_cspeed(filter_duration(finished_paths), count_cutoff=30, scaling='standard').sort_values(by='standard', ascending=False)\n",
    "cs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In sampling a total of 1523 samples were removed, which represents 5.504% of the original data. 27672 samples remain.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sj/219f9qbn3y15yynshcyrrp3c0000gn/T/ipykernel_10877/1974316627.py:117: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '-12.166165380948254' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  sum_cspeed_df.at[article, 'sum_cspeed'] += cspeed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique articles after speed calc: 517\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_appearances</th>\n",
       "      <th>sum_cspeed</th>\n",
       "      <th>standard</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Europe</th>\n",
       "      <td>2070</td>\n",
       "      <td>-4266.034777</td>\n",
       "      <td>10.397022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>North_America</th>\n",
       "      <td>789</td>\n",
       "      <td>-2596.390597</td>\n",
       "      <td>6.306012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Earth</th>\n",
       "      <td>1137</td>\n",
       "      <td>-2502.383898</td>\n",
       "      <td>6.075674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asia</th>\n",
       "      <td>588</td>\n",
       "      <td>-1947.121514</td>\n",
       "      <td>4.715155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Continent</th>\n",
       "      <td>294</td>\n",
       "      <td>-1720.124051</td>\n",
       "      <td>4.158959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Computer</th>\n",
       "      <td>441</td>\n",
       "      <td>1052.565259</td>\n",
       "      <td>-2.634763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>British_Empire</th>\n",
       "      <td>313</td>\n",
       "      <td>1209.835077</td>\n",
       "      <td>-3.020110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United_Kingdom</th>\n",
       "      <td>1948</td>\n",
       "      <td>1399.884008</td>\n",
       "      <td>-3.485773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United_States</th>\n",
       "      <td>4286</td>\n",
       "      <td>2188.212724</td>\n",
       "      <td>-5.417358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>World_War_II</th>\n",
       "      <td>1013</td>\n",
       "      <td>2220.625696</td>\n",
       "      <td>-5.496778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                n_appearances   sum_cspeed   standard\n",
       "article                                              \n",
       "Europe                   2070 -4266.034777  10.397022\n",
       "North_America             789 -2596.390597   6.306012\n",
       "Earth                    1137 -2502.383898   6.075674\n",
       "Asia                      588 -1947.121514   4.715155\n",
       "Continent                 294 -1720.124051   4.158959\n",
       "...                       ...          ...        ...\n",
       "Computer                  441  1052.565259  -2.634763\n",
       "British_Empire            313  1209.835077  -3.020110\n",
       "United_Kingdom           1948  1399.884008  -3.485773\n",
       "United_States            4286  2188.212724  -5.417358\n",
       "World_War_II             1013  2220.625696  -5.496778\n",
       "\n",
       "[517 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs2 = calc_sum_article_cspeed(filter_duration(filtered_paths_sampled), count_cutoff=30, scaling='standard').sort_values(by='standard', ascending=False)\n",
    "cs2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADAproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
